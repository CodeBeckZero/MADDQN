{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in another environment (e.g., VS Code)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Code specific to Google Colab\n",
    "    print(\"Running in Google Colab\")\n",
    "\n",
    "    # Additional setup commands for Colab\n",
    "    !pip install neuralforecast\n",
    "    !pip install gymnasium\n",
    "    !pip install QuantStats\n",
    "else:\n",
    "    # Code for other environments (e.g., VS Code)\n",
    "    print(\"Running in another environment (e.g., VS Code)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install RL Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Retrive required files\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/environments/stockenv.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/cleandata.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/data.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/epsilon_decay.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/agentperform.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/prob_evaluate.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/ddqn.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/random.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/baseagent.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/rewards/stockmarket.py            \n",
    "    # Move all directories and files from content/raw.githubusercontent.com to content/\n",
    "    !mv /content/raw.githubusercontent.com/* /content/\n",
    "\n",
    "    # Delete the raw.githubusercontent.com directory\n",
    "    !rm -rf /content/raw.githubusercontent.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activate Python Libraries & Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is not available. PyTorch is running on CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import optuna\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import utilities.agentperform as agentperform\n",
    "import utilities.cleandata as cln \n",
    "from utilities.epsilon_decay import linear_decay\n",
    "from utilities.data import UniStockEnvDataStruct, TimesNetProcessing, ModifyDDQNAgentState\n",
    "from utilities import prob_evaluate\n",
    "from agents.ddqn import DdqnAgent\n",
    "from agents.random import RandomAgent\n",
    "from rewards.stockmarket import future_profit, risk_reward, zero_reward\n",
    "from environments.stockenv import ContinuousOHLCVEnv\n",
    "from datetime import datetime\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import TimesNet\n",
    "from neuralforecast.losses.numpy import mae, mse\n",
    "import logging\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# \n",
    "logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").addHandler(logging.NullHandler())\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").addHandler(logging.NullHandler())\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1' # Prevent Warning \n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    # Python random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # If you are using CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "        # Additional settings to force determinism in your operations:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"CUDA (GPU support) is not available. PyTorch is running on CPU.\")\n",
    "\n",
    "\n",
    "def decimal_to_text(decimal_number):\n",
    "    # Remove the decimal point and convert to integer\n",
    "    integer_part = int(decimal_number * 1000)\n",
    "    # Convert the integer to text\n",
    "    text_representation = str(integer_part)\n",
    "    return text_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters & CSV Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "\n",
    "if not IN_COLAB:\n",
    "    pwd = \"C:/programming/MADDQN\"\n",
    "    sys.path.append(pwd)\n",
    "    \n",
    "    # Output Path Location for CSV export\n",
    "    export_path = pwd + \"/output_data/test001/\"\n",
    "\n",
    "# Input Data Location, File Name, Stock name for labels\n",
    "input_url = 'https://raw.githubusercontent.com/CodeBeckZero/MADDQN/main/input_data'\n",
    "\n",
    "stock_inputs ={'DJI':'^DJI_daily.csv',\n",
    "               'NDAQ': '^IXIC_daily.csv',\n",
    "               'SP500': '^SPX_daily.csv',\n",
    "               'AAPL': 'AAPL_daily.csv',\n",
    "               'AMZN': 'AMZN_daily.csv',\n",
    "               'GOOGL': 'GOOGL_daily.csv',\n",
    "               'MSFT': 'MSFT_daily.csv',\n",
    "               'SINE': 'sine_wave_daily.csv',\n",
    "               'FORD': 'F_daily.csv',\n",
    "               'JNJ': 'JNJ_daily.csv',\n",
    "               'NEE': 'NEE_daily.csv',\n",
    "               'PFE': 'PFE_daily.csv',\n",
    "               'TSLA': 'TSLA_daily.csv',\n",
    "               'COKE': 'COKE_daily.csv',\n",
    "               'PG': 'PG_daily.csv'}\n",
    "\n",
    "# Training Inputs\n",
    "trn_keys = ['DJI']#,'NDAQ','SP500']\n",
    "\n",
    "# Validation Inputs\n",
    "val_keys = trn_keys\n",
    "\n",
    "# Testing Inputs\n",
    "tst_keys = ['AAPL','AMZN','GOOGL']#,'MSFT','FORD','JNJ','NEE','PFE','TSLA','COKE','PG']\n",
    "\n",
    "window_size = 28 # Needs to match the size Timesnet is trained on\n",
    "price_based_on = 'close'\n",
    "columns = ['open','high','low','close','volume']\n",
    "\n",
    "\n",
    "# Metrics Interested in\n",
    "metrics = ['n_trades','n_wins', 'win_percentage','cumulative_return','sortino','max_drawdown','sharpe', 'trade_dur_avg']\n",
    "\n",
    "aval_metrics_rank_dic = {'n_trades':'max','n_wins': 'max' ,'n_losses':'max','win_percentage':'max','cumulative_return':'max', \n",
    "                 'sortino':'max','max_drawdown':'min', 'sharpe':'max', 'trade_dur_avg':'max', 'trade_dur_min':'max',\n",
    "                 'trade_dur_max':'max','buy_hold':'max'}\n",
    "## See agentperform.py -> results dictionary for options\n",
    "\n",
    "env_mod_parms = {'columns': columns, 'scaling_type': 'col', 'scaler_func':preprocessing.StandardScaler()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Enviornment Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_data = {}\n",
    "env = {}\n",
    "\n",
    "\n",
    "for stock, file in stock_inputs.items():\n",
    "    if stock in set(trn_keys + val_keys + tst_keys):\n",
    "        # Import\n",
    "        df = cln.YAHOO_csv_input(file, input_url)\n",
    "        data_dic = UniStockEnvDataStruct(df,columns,price_based_on,window_size)\n",
    "        env_data[stock] = data_dic\n",
    "        env[stock] = ContinuousOHLCVEnv(name=stock,\n",
    "                                        ohlcv_data = env_data[stock]['rw_raw_env'] ,\n",
    "                                        stock_price_data= env_data[stock]['rw_raw_price_env'],\n",
    "                                        commission_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-01-03</td>\n",
       "      <td>12459.54</td>\n",
       "      <td>12580.35</td>\n",
       "      <td>12404.82</td>\n",
       "      <td>12474.52</td>\n",
       "      <td>327200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-01-04</td>\n",
       "      <td>12473.16</td>\n",
       "      <td>12510.41</td>\n",
       "      <td>12403.86</td>\n",
       "      <td>12480.69</td>\n",
       "      <td>259060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-01-05</td>\n",
       "      <td>12480.05</td>\n",
       "      <td>12480.13</td>\n",
       "      <td>12365.41</td>\n",
       "      <td>12398.01</td>\n",
       "      <td>235220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-01-08</td>\n",
       "      <td>12392.01</td>\n",
       "      <td>12445.92</td>\n",
       "      <td>12337.37</td>\n",
       "      <td>12423.49</td>\n",
       "      <td>223500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-01-09</td>\n",
       "      <td>12424.77</td>\n",
       "      <td>12466.43</td>\n",
       "      <td>12369.17</td>\n",
       "      <td>12416.60</td>\n",
       "      <td>225190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2007-01-10</td>\n",
       "      <td>12417.00</td>\n",
       "      <td>12451.61</td>\n",
       "      <td>12355.63</td>\n",
       "      <td>12442.16</td>\n",
       "      <td>226570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2007-01-11</td>\n",
       "      <td>12442.96</td>\n",
       "      <td>12544.46</td>\n",
       "      <td>12442.96</td>\n",
       "      <td>12514.98</td>\n",
       "      <td>261720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007-01-12</td>\n",
       "      <td>12514.66</td>\n",
       "      <td>12561.04</td>\n",
       "      <td>12489.66</td>\n",
       "      <td>12556.08</td>\n",
       "      <td>256530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2007-01-16</td>\n",
       "      <td>12555.84</td>\n",
       "      <td>12585.08</td>\n",
       "      <td>12538.93</td>\n",
       "      <td>12582.59</td>\n",
       "      <td>242720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2007-01-17</td>\n",
       "      <td>12571.46</td>\n",
       "      <td>12614.00</td>\n",
       "      <td>12550.55</td>\n",
       "      <td>12577.15</td>\n",
       "      <td>272720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2007-01-18</td>\n",
       "      <td>12575.06</td>\n",
       "      <td>12611.91</td>\n",
       "      <td>12547.34</td>\n",
       "      <td>12567.93</td>\n",
       "      <td>250690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2007-01-19</td>\n",
       "      <td>12567.93</td>\n",
       "      <td>12586.84</td>\n",
       "      <td>12523.55</td>\n",
       "      <td>12565.53</td>\n",
       "      <td>287480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2007-01-22</td>\n",
       "      <td>12565.53</td>\n",
       "      <td>12572.58</td>\n",
       "      <td>12450.89</td>\n",
       "      <td>12477.16</td>\n",
       "      <td>293400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2007-01-23</td>\n",
       "      <td>12477.81</td>\n",
       "      <td>12554.23</td>\n",
       "      <td>12468.35</td>\n",
       "      <td>12533.80</td>\n",
       "      <td>236760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2007-01-24</td>\n",
       "      <td>12534.37</td>\n",
       "      <td>12623.45</td>\n",
       "      <td>12531.08</td>\n",
       "      <td>12621.77</td>\n",
       "      <td>216920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2007-01-25</td>\n",
       "      <td>12621.77</td>\n",
       "      <td>12622.65</td>\n",
       "      <td>12487.34</td>\n",
       "      <td>12502.56</td>\n",
       "      <td>275780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2007-01-26</td>\n",
       "      <td>12503.28</td>\n",
       "      <td>12539.09</td>\n",
       "      <td>12431.34</td>\n",
       "      <td>12487.02</td>\n",
       "      <td>247020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2007-01-29</td>\n",
       "      <td>12487.10</td>\n",
       "      <td>12542.70</td>\n",
       "      <td>12481.49</td>\n",
       "      <td>12490.78</td>\n",
       "      <td>234510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2007-01-30</td>\n",
       "      <td>12492.23</td>\n",
       "      <td>12538.45</td>\n",
       "      <td>12459.46</td>\n",
       "      <td>12523.31</td>\n",
       "      <td>244040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2007-01-31</td>\n",
       "      <td>12520.03</td>\n",
       "      <td>12657.02</td>\n",
       "      <td>12505.20</td>\n",
       "      <td>12621.69</td>\n",
       "      <td>258410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2007-02-01</td>\n",
       "      <td>12617.20</td>\n",
       "      <td>12682.57</td>\n",
       "      <td>12616.08</td>\n",
       "      <td>12673.68</td>\n",
       "      <td>235130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2007-02-02</td>\n",
       "      <td>12673.84</td>\n",
       "      <td>12683.93</td>\n",
       "      <td>12638.35</td>\n",
       "      <td>12653.49</td>\n",
       "      <td>203610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2007-02-05</td>\n",
       "      <td>12653.41</td>\n",
       "      <td>12681.21</td>\n",
       "      <td>12629.86</td>\n",
       "      <td>12661.74</td>\n",
       "      <td>204140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2007-02-06</td>\n",
       "      <td>12661.66</td>\n",
       "      <td>12680.57</td>\n",
       "      <td>12633.94</td>\n",
       "      <td>12666.31</td>\n",
       "      <td>201010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2007-02-07</td>\n",
       "      <td>12656.86</td>\n",
       "      <td>12700.28</td>\n",
       "      <td>12630.50</td>\n",
       "      <td>12666.87</td>\n",
       "      <td>194020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2007-02-08</td>\n",
       "      <td>12665.67</td>\n",
       "      <td>12665.83</td>\n",
       "      <td>12575.86</td>\n",
       "      <td>12637.63</td>\n",
       "      <td>193820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2007-02-09</td>\n",
       "      <td>12638.03</td>\n",
       "      <td>12675.68</td>\n",
       "      <td>12545.10</td>\n",
       "      <td>12580.83</td>\n",
       "      <td>220330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2007-02-12</td>\n",
       "      <td>12580.11</td>\n",
       "      <td>12607.51</td>\n",
       "      <td>12536.21</td>\n",
       "      <td>12552.55</td>\n",
       "      <td>174980000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      open      high       low     close     volume\n",
       "0  2007-01-03  12459.54  12580.35  12404.82  12474.52  327200000\n",
       "1  2007-01-04  12473.16  12510.41  12403.86  12480.69  259060000\n",
       "2  2007-01-05  12480.05  12480.13  12365.41  12398.01  235220000\n",
       "3  2007-01-08  12392.01  12445.92  12337.37  12423.49  223500000\n",
       "4  2007-01-09  12424.77  12466.43  12369.17  12416.60  225190000\n",
       "5  2007-01-10  12417.00  12451.61  12355.63  12442.16  226570000\n",
       "6  2007-01-11  12442.96  12544.46  12442.96  12514.98  261720000\n",
       "7  2007-01-12  12514.66  12561.04  12489.66  12556.08  256530000\n",
       "8  2007-01-16  12555.84  12585.08  12538.93  12582.59  242720000\n",
       "9  2007-01-17  12571.46  12614.00  12550.55  12577.15  272720000\n",
       "10 2007-01-18  12575.06  12611.91  12547.34  12567.93  250690000\n",
       "11 2007-01-19  12567.93  12586.84  12523.55  12565.53  287480000\n",
       "12 2007-01-22  12565.53  12572.58  12450.89  12477.16  293400000\n",
       "13 2007-01-23  12477.81  12554.23  12468.35  12533.80  236760000\n",
       "14 2007-01-24  12534.37  12623.45  12531.08  12621.77  216920000\n",
       "15 2007-01-25  12621.77  12622.65  12487.34  12502.56  275780000\n",
       "16 2007-01-26  12503.28  12539.09  12431.34  12487.02  247020000\n",
       "17 2007-01-29  12487.10  12542.70  12481.49  12490.78  234510000\n",
       "18 2007-01-30  12492.23  12538.45  12459.46  12523.31  244040000\n",
       "19 2007-01-31  12520.03  12657.02  12505.20  12621.69  258410000\n",
       "20 2007-02-01  12617.20  12682.57  12616.08  12673.68  235130000\n",
       "21 2007-02-02  12673.84  12683.93  12638.35  12653.49  203610000\n",
       "22 2007-02-05  12653.41  12681.21  12629.86  12661.74  204140000\n",
       "23 2007-02-06  12661.66  12680.57  12633.94  12666.31  201010000\n",
       "24 2007-02-07  12656.86  12700.28  12630.50  12666.87  194020000\n",
       "25 2007-02-08  12665.67  12665.83  12575.86  12637.63  193820000\n",
       "26 2007-02-09  12638.03  12675.68  12545.10  12580.83  220330000\n",
       "27 2007-02-12  12580.11  12607.51  12536.21  12552.55  174980000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.245954e+04, 1.258035e+04, 1.240482e+04, 1.247452e+04,\n",
       "         3.272000e+08],\n",
       "        [1.247316e+04, 1.251041e+04, 1.240386e+04, 1.248069e+04,\n",
       "         2.590600e+08],\n",
       "        [1.248005e+04, 1.248013e+04, 1.236541e+04, 1.239801e+04,\n",
       "         2.352200e+08],\n",
       "        [1.239201e+04, 1.244592e+04, 1.233737e+04, 1.242349e+04,\n",
       "         2.235000e+08],\n",
       "        [1.242477e+04, 1.246643e+04, 1.236917e+04, 1.241660e+04,\n",
       "         2.251900e+08],\n",
       "        [1.241700e+04, 1.245161e+04, 1.235563e+04, 1.244216e+04,\n",
       "         2.265700e+08],\n",
       "        [1.244296e+04, 1.254446e+04, 1.244296e+04, 1.251498e+04,\n",
       "         2.617200e+08],\n",
       "        [1.251466e+04, 1.256104e+04, 1.248966e+04, 1.255608e+04,\n",
       "         2.565300e+08],\n",
       "        [1.255584e+04, 1.258508e+04, 1.253893e+04, 1.258259e+04,\n",
       "         2.427200e+08],\n",
       "        [1.257146e+04, 1.261400e+04, 1.255055e+04, 1.257715e+04,\n",
       "         2.727200e+08],\n",
       "        [1.257506e+04, 1.261191e+04, 1.254734e+04, 1.256793e+04,\n",
       "         2.506900e+08],\n",
       "        [1.256793e+04, 1.258684e+04, 1.252355e+04, 1.256553e+04,\n",
       "         2.874800e+08],\n",
       "        [1.256553e+04, 1.257258e+04, 1.245089e+04, 1.247716e+04,\n",
       "         2.934000e+08],\n",
       "        [1.247781e+04, 1.255423e+04, 1.246835e+04, 1.253380e+04,\n",
       "         2.367600e+08],\n",
       "        [1.253437e+04, 1.262345e+04, 1.253108e+04, 1.262177e+04,\n",
       "         2.169200e+08],\n",
       "        [1.262177e+04, 1.262265e+04, 1.248734e+04, 1.250256e+04,\n",
       "         2.757800e+08],\n",
       "        [1.250328e+04, 1.253909e+04, 1.243134e+04, 1.248702e+04,\n",
       "         2.470200e+08],\n",
       "        [1.248710e+04, 1.254270e+04, 1.248149e+04, 1.249078e+04,\n",
       "         2.345100e+08],\n",
       "        [1.249223e+04, 1.253845e+04, 1.245946e+04, 1.252331e+04,\n",
       "         2.440400e+08],\n",
       "        [1.252003e+04, 1.265702e+04, 1.250520e+04, 1.262169e+04,\n",
       "         2.584100e+08],\n",
       "        [1.261720e+04, 1.268257e+04, 1.261608e+04, 1.267368e+04,\n",
       "         2.351300e+08],\n",
       "        [1.267384e+04, 1.268393e+04, 1.263835e+04, 1.265349e+04,\n",
       "         2.036100e+08],\n",
       "        [1.265341e+04, 1.268121e+04, 1.262986e+04, 1.266174e+04,\n",
       "         2.041400e+08],\n",
       "        [1.266166e+04, 1.268057e+04, 1.263394e+04, 1.266631e+04,\n",
       "         2.010100e+08],\n",
       "        [1.265686e+04, 1.270028e+04, 1.263050e+04, 1.266687e+04,\n",
       "         1.940200e+08],\n",
       "        [1.266567e+04, 1.266583e+04, 1.257586e+04, 1.263763e+04,\n",
       "         1.938200e+08],\n",
       "        [1.263803e+04, 1.267568e+04, 1.254510e+04, 1.258083e+04,\n",
       "         2.203300e+08],\n",
       "        [1.258011e+04, 1.260751e+04, 1.253621e+04, 1.255255e+04,\n",
       "         1.749800e+08]]),\n",
       " 0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.44441198288491807,\n",
       " 0.26317264978117594,\n",
       " 0.4326994762109525,\n",
       " 0.05319799840181585,\n",
       " -1.9363004055412818,\n",
       " 0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.44441198288491807,\n",
       " 0.26317264978117594,\n",
       " 0.4326994762109525,\n",
       " 0.05319799840181585,\n",
       " -1.9363004055412818]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaler = ModifyDDQNAgentState(env_data['DJI'],columns=columns,scaling_type='col',scaler_func=preprocessing.StandardScaler())\n",
    "new_state =[]\n",
    "\n",
    "for idx, col in enumerate(columns):\n",
    "    x = env_data['DJI']['rw_raw_df'][0][col].to_numpy().reshape(-1,1)\n",
    "    y = scaler.scaler[idx].fit_transform(x)\n",
    "   \n",
    "    x_tst = x.flatten()\n",
    "    y_tst = y.flatten()\n",
    "    new_state.append(y_tst[-1])\n",
    "   \n",
    "display(env_data['DJI']['rw_raw_df'][0])\n",
    "display(env['DJI'].get_observation())\n",
    "y = scaler.process(env['DJI'])\n",
    "display(y, new_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbench Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Directory\n",
    "case_name = '/test001'\n",
    "save_path_root = pwd + case_name\n",
    "os.makedirs(save_path_root, exist_ok=False)\n",
    "\n",
    "# Timesnet\n",
    "## Number of price predictions in the Future by TimesNet (Required for RL agent's input layer)\n",
    "n_prediction = 5\n",
    "## Need to Train TimesNet Preprocessing model (processed every cycle)                            \n",
    "train_tn_model = False \n",
    "## Importing TimesNet Preprocessing model (processed every cycle)            \n",
    "import_tn_model = False                     \n",
    "tn_model_path = pwd + '/gen_data/timesnet/'\n",
    "## Use Imported CSVs from Preprocessing model (no processing, straight to RL agent)\n",
    "import_tn_csvs = True                       \n",
    "tn_csvs_path = pwd + '/gen_data/csvs/'   \n",
    "## No modifaction of environmental state before input to agent\n",
    "no_tn_preprocessing = False                \n",
    "\n",
    "# Limited Exploratory Hyperparmater Discover for single RL Agent\n",
    "hyperparam_discovery = False               \n",
    "\n",
    "# Traditional Training/Testing\n",
    "## Train Agent(s)\n",
    "train_agent = True                          \n",
    "# Test Agent(s)\n",
    "test_agent = True  \n",
    "\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  not(train_tn_model ^ import_tn_model ^ import_tn_csvs ^ no_tn_preprocessing):\n",
    "    raise ValueError(\"Only one preprocessing options can and must be selected\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Function\n",
    "\n",
    "Function that generates metric from enviornment that will be used during validation phase of training or testing phase of model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_function(env):\n",
    "    metric = env.step_info[-1]['New Portfolio Value'] -  env.step_info[-1]['Portfolio Value']\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimesNet Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_tn_model:\n",
    "    \n",
    "    model = TimesNet(h = n_prediction,          # Forecast horizon\n",
    "                    input_size = window_size,   # Length of Batches\n",
    "                    batch_size = 1,             # Number of timeseries to predict\n",
    "                    #futr_exog_list = remaining_columns,\n",
    "                    hidden_size = 128,          # Size of embedding for embedding and encoders,\n",
    "                    dropout = 0.40,             # Dropout for embeddings\n",
    "                    conv_hidden_size = 3,       # Channels for the inception block\n",
    "                    top_k = 5,                  # Top num of periods from FFT considered\n",
    "                    num_kernels = 13,           # number of kernels for the inception block\n",
    "                    encoder_layers = 3,         # num of encoders\n",
    "                    max_steps = 1000,           # of training steps\n",
    "                    early_stop_patience_steps = 10, #early stoppage on validation\n",
    "                    val_check_steps = 100,      # Val check every X steps,\n",
    "                    windows_batch_size = 150,   # Number of windows in training epoch,\n",
    "                    num_workers_loader = 7,\n",
    "                    learning_rate = 0.0003,\n",
    "                    random_seed = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_tn_model:\n",
    "  nf = NeuralForecast(models=[model], freq='d')\n",
    "  results = {}\n",
    "  for key in trn_keys:\n",
    "    results[key] = nf.fit(df=env[key],val_size=0.2)\n",
    "\n",
    "  nf.save(path= tn_model_path,\n",
    "          model_index=None,\n",
    "          overwrite=True,\n",
    "          save_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_tn_model:\n",
    "# Define the correct path\n",
    "  if IN_COLAB:\n",
    "    \n",
    "    model_path = os.path.join(os.getcwd(), 'gen_data', 'timesnet')\n",
    "\n",
    "    # Ensure the directory and file exist\n",
    "    if os.path.exists(model_path):\n",
    "        nf = NeuralForecast.load(path=model_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model path {model_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_tn_csvs:\n",
    "    env_mod_func_dic = {}\n",
    "    if IN_COLAB:\n",
    "        # Input Data Location, File Name, Stock name for labels\n",
    "        csv_path = 'https://raw.githubusercontent.com/CodeBeckZero/MADDQN/main/gen_data/csvs/'\n",
    "\n",
    "    else:\n",
    "        csv_path  = tn_csvs_path\n",
    "\n",
    "    stock_tn ={'DJI':'DJI_tn.csv',\n",
    "                'NDAQ': 'NDAQ_tn.csv',\n",
    "                'SP500': 'SP500_tn.csv',\n",
    "                'AAPL': 'AAPL_tn.csv',\n",
    "                'AMZN': 'AMZN_tn.csv',\n",
    "                'GOOGL': 'GOOGL_tn.csv',\n",
    "                'MSFT': 'MSFT_tn.csv',\n",
    "                'FORD': 'FORD_tn.csv',\n",
    "                'JNJ': 'JNJ_tn.csv',\n",
    "                'NEE': 'NEE_tn.csv',\n",
    "                'PFE': 'PFE_tn.csv',\n",
    "                'TSLA': 'TSLA_tn.csv',\n",
    "                'COKE': 'COKE_tn.csv',\n",
    "                'PG': 'PG_tn.csv'}\n",
    "    \n",
    "    for stock in set(trn_keys + val_keys + tst_keys):\n",
    "        temp_env_mod_fuc = ModifyDDQNAgentState(uni_data=env_data[stock],\n",
    "                                                columns=columns,\n",
    "                                                csv_import=import_tn_csvs,\n",
    "                                                csv_path=csv_path,\n",
    "                                                scaling_type='col',\n",
    "                                                scaler_func=preprocessing.StandardScaler())\n",
    "        env_mod_func_dic[stock] = temp_env_mod_fuc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct\n",
    "Current direct default is to take the rolling windows last entry and convert it to a list prior to input to agent. Agent's experience memory is currently list-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Hyperparameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparam_discovery:\n",
    "    # Training Inputs\n",
    "    hyp_training_range = ('2007-01-01','2020-12-31')\n",
    "    hyp_trn_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_training_range]\n",
    "\n",
    "    # Validation Inputs\n",
    "    hyp_validation_range = ('2021-01-01', '2021-12-31')\n",
    "    hyp_val_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_validation_range]\n",
    "\n",
    "    # Testing Inputs\n",
    "    hyp_testing_range = ('2021-01-01', '2023-12-31')\n",
    "    hyp_tst_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_testing_range]\n",
    "\n",
    "    hyp_trn_idx = {}\n",
    "    hyp_val_idx = {}\n",
    "    hyp_tst_idx = {}\n",
    "\n",
    "    for stock, file in stock_inputs.items():\n",
    "        if stock in set(trn_keys + val_keys + tst_keys):\n",
    "            if stock in trn_keys:\n",
    "                hyp_trn_idx[stock] = env_data[stock].gen_rw_idxs(hyp_trn_dt_range)\n",
    "            if stock in val_keys:\n",
    "                hyp_val_idx[stock] = env_data[stock].gen_rw_idxs(hyp_val_dt_range)\n",
    "            if stock in tst_keys:\n",
    "                hyp_tst_idx[stock] = env_data[stock].gen_rw_idxs(hyp_tst_dt_range)\n",
    "\n",
    "    display(hyp_trn_idx,hyp_val_idx,hyp_tst_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Search & Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparam_discovery:\n",
    "    \n",
    "    # For Objective function, need to create agent name before to link agent with enviornment\n",
    "    agent_name = 'hyp_discovery_agent'\n",
    "    agent_path = export_path + '/' + agent_name\n",
    "    metric = 'val_tot_r'\n",
    "    max_len_buf = np.round(hyp_trn_idx['DJI'][1] - hyp_trn_idx['DJI'][0] + window_size, -2) -10 # manual input, could be error here if \n",
    "    print(f'Max Mem Length: {max_len_buf}')\n",
    "           \n",
    "    def objective(trial):\n",
    "    \n",
    "        # Define the hyperparameters to search over\n",
    "        \n",
    "        ## NN hyperparameters\n",
    "        sug_hidden_layers = trial.suggest_int('hidden_layers', low=1, high=3)\n",
    "        sug_hidden_size = trial.suggest_int('hidden_size', low=64, high=512, step=64)\n",
    "        sug_update_q_freq = trial.suggest_int('update_q_freq',low=1,high=5)\n",
    "        sug_update_tgt_freq = trial.suggest_int('update_tgt_freq',low=5,high=15)\n",
    "        \n",
    "        ## Activation Function Passing\n",
    "        activation_functions = {\n",
    "        'LRELUd': nn.LeakyReLU(),\n",
    "        'LRELUs02': nn.LeakyReLU(negative_slope=0.2),\n",
    "        'GELU': nn.GELU(),\n",
    "        'TANH': nn.Tanh(),\n",
    "        'SELU':nn.SELU(),\n",
    "        'SILU': nn.SiLU()\n",
    "        }\n",
    "        sug_activation_function_name = trial.suggest_categorical('activation_function', list(activation_functions.keys()))\n",
    "        sug_activation_function = activation_functions[sug_activation_function_name]\n",
    "        \n",
    "        \"\"\"\n",
    "        ## Reward Function Passing\n",
    "        reward_functions = {\n",
    "        'profit': future_profit(None,5),\n",
    "        'risk': risk_reward(None,5),\n",
    "        }\n",
    "        sug_reward_function_name = trial.suggest_categorical('reward_function', list(reward_functions.keys()))\n",
    "        sug_reward_function = reward_functions[sug_reward_function_name]\n",
    "        \"\"\"\n",
    "        ## Optimizer hyperparameters\n",
    "        sug_opt_lre = trial.suggest_categorical('opt_lre',[0.0001,0.0005,0.001, 0.005, 0.01, 0.05, 0.1])\n",
    "        sug_gamma = trial.suggest_float('gamma',low=0.90,high=0.99,step=0.01)\n",
    "        ## Memory Replay hyperparameters\n",
    "        sug_buffer_size = trial.suggest_int('buffer_size',low=100,high=max_len_buf,step=10)\n",
    "        sug_batch_size = trial.suggest_int('batch_size',low=10,high=sug_buffer_size,step=5)\n",
    "        \n",
    "        # Saving Setup\n",
    "        ## Current Parameter Values:\n",
    "        cur_n_fcl = trial.params['hidden_layers']\n",
    "        cur_fcl_size = trial.params['hidden_size']\n",
    "        cur_q_freq = trial.params['update_q_freq']\n",
    "        cur_tgt_freq = trial.params['update_tgt_freq']\n",
    "        cur_act_func = trial.params['activation_function']\n",
    "        #cur_rwd_func = trial.params['reward_function']\n",
    "        cur_lre = decimal_to_text(trial.params['opt_lre'])\n",
    "        cur_buf_size = trial.params['buffer_size']\n",
    "        cur_bat_size = trial.params['batch_size']\n",
    "        \n",
    "        ## Create Notation for Hyperparameter Setup    \n",
    "        test_name = (f'{cur_n_fcl}FC{cur_fcl_size}_{cur_act_func}_' +\n",
    "                    f'BT{cur_bat_size}BF{cur_buf_size}_Q{cur_q_freq}_' +\n",
    "                    f'TGT{cur_tgt_freq}_LR{cur_lre}')\n",
    "        \n",
    "        ## Create Dir to save results\n",
    "        test_name_path =  agent_path + '/' + test_name \n",
    "        if not os.path.exists(test_name_path):\n",
    "            os.makedirs(test_name_path)\n",
    "            print(f\"Directory '{test_name_path}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Directory '{test_name_path}' already exists.\")\n",
    "        \n",
    "        # Create Agent with hyperparameters  \n",
    "        best_agent = DdqnAgent(name=agent_name,\n",
    "                            environment=None,\n",
    "                            reward_function = future_profit,\n",
    "                            reward_params = {'n':5},\n",
    "                            env_state_mod_func = env_mod_func,     \n",
    "                            input_size= 11,\n",
    "                            hidden_size= sug_hidden_size, \n",
    "                            output_size=3, \n",
    "                            activation_function = sug_activation_function,\n",
    "                            num_hidden_layers = sug_hidden_layers,                  \n",
    "                            buffer_size= sug_buffer_size, \n",
    "                            batch_size = sug_batch_size,\n",
    "                            alpha = sug_opt_lre,\n",
    "                            gamma = sug_gamma,\n",
    "                            opt_wgt_dcy = 0.01,\n",
    "                            dropout_rate = 0.25,                \n",
    "                            device = device)\n",
    "        \n",
    "        # Training Model\n",
    "        for key, rl_env in env.items():\n",
    "            \n",
    "            if key in trn_keys:\n",
    "                rl_env.add_agent(agent_name)\n",
    "                rl_env.set_decision_agent(agent_name)\n",
    "                if import_tn_csvs:\n",
    "                    timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')    #Requires outside variable         \n",
    "                best_agent.set_environment(rl_env)\n",
    "                best_agent.train(start_idx=hyp_trn_idx[key][0],\n",
    "                            end_idx=hyp_trn_idx[key][1],\n",
    "                            training_episodes= 30,\n",
    "                            epsilon_decya_func= linear_decay,\n",
    "                            initial_epsilon= 0.9,\n",
    "                            final_epsilon= 0.1,\n",
    "                            update_q_freq= sug_update_q_freq,\n",
    "                            update_tgt_freq= sug_update_tgt_freq,\n",
    "                            save_path = export_path,\n",
    "                            val_start_idx = hyp_val_idx[key][0],\n",
    "                            val_end_idx = hyp_val_idx[key][1],\n",
    "                            metric_func= metric_function,\n",
    "                            min_training_episodes = 1, \n",
    "                            early_stop = True,\n",
    "                            stop_metric = metric,\n",
    "                            stop_patience = 15,\n",
    "                            stop_delta = 0.001)\n",
    "                rl_env.remove_agent(agent_name)\n",
    "\n",
    "        # Test Model\n",
    "        \n",
    "        scores = []\n",
    "        for key, rl_env in env.items():\n",
    "        \n",
    "            if key in tst_keys:\n",
    "                rl_env.add_agent(agent_name)\n",
    "                rl_env.set_decision_agent(agent_name)\n",
    "                if import_tn_csvs:\n",
    "                    timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')    #Requires outside variable              \n",
    "                best_agent.set_environment(rl_env)              \n",
    "                best_agent.test(start_idx = hyp_tst_idx[key][0],\n",
    "                            end_idx = hyp_tst_idx[key][1],\n",
    "                            metric_func= metric_function, \n",
    "                            testing_episodes=1)\n",
    "                rl_env.remove_agent(agent_name)\n",
    "\n",
    "                ## Save Test Metric Result(s) into \n",
    "                ddqn_tst = best_agent.get_testing_episodic_data()\n",
    "                score = ddqn_tst['tot_r'].mean()\n",
    "                scores.append(score)\n",
    "        \n",
    "                ## Export Test data\n",
    "                a = rl_env.get_step_data()\n",
    "                b = best_agent.get_step_data()\n",
    "                combined_df = pd.concat([a,b],axis=1)\n",
    "                tst_df_file_name  = f'TST-{key}' + test_name + '.csv'\n",
    "                trn_df_save_path = test_name_path + '/' + tst_df_file_name\n",
    "                combined_df.to_csv(trn_df_save_path)\n",
    "\n",
    "                ## Generate Trading Graphic\n",
    "                tst_graph_file_name = trn_df_save_path[:-4] + '.png'\n",
    "                agentperform.agent_stock_performance(env[key].stock_price_data[hyp_tst_idx[key][0]:hyp_tst_idx[key][1]][:,-1,0], # Selecting all batches, last price of window, closing price\n",
    "                                                    combined_df['Env Action'].to_numpy(),\n",
    "                                                    key,\n",
    "                                                    best_agent.get_name(),\n",
    "                                                    display_graph=False,\n",
    "                                                    save_graphic=True,\n",
    "                                                    path_file=tst_graph_file_name)\n",
    "\n",
    "        mean = np.mean(scores)\n",
    "        return mean\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"Best value: \", study.best_value)\n",
    "    print(\"Best params: \", study.best_params)\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Type Setup\n",
    "agent_classes = {'profit': DdqnAgent,\n",
    "                 'risk': DdqnAgent,\n",
    "                 'random':RandomAgent}\n",
    "\n",
    "# Mul\n",
    "agent_setup = {'profit': ['profit'],\n",
    "                 'risk': ['risk'],\n",
    "                 'random': ['random']}\n",
    "                 #final': ['profit', 'risk'], for multi agent key is decision agent\n",
    "                 #'macro': 'macro', \n",
    "                 #'opt': ['profit', 'risk', 'macro']}\n",
    "                 \n",
    "agent_name_list = list(agent_classes.keys())\n",
    "\n",
    "agents_to_train = ['profit', 'risk']\n",
    "agents_to_import = {'agent_name': 'path_to_model'}\n",
    "\n",
    "agent_params = {\n",
    "    agent_name_list[0]:{\n",
    "        'name': agent_name_list[0],\n",
    "        'environment': None,\n",
    "        'reward_function': future_profit,\n",
    "        'reward_params': {'n':5},\n",
    "        'env_state_mod_func': env_mod_func,\n",
    "        'input_size': 11,\n",
    "        'hidden_size': 256,\n",
    "        'output_size':3,\n",
    "        'activation_function': nn.GELU(),\n",
    "        'num_hidden_layers': 3,\n",
    "        'buffer_size': 330,\n",
    "        'batch_size': 75,\n",
    "        'alpha': 0.0005,\n",
    "        'gamma':0.96,\n",
    "        'opt_wgt_dcy': 0.001,\n",
    "        'dropout_rate': 0.25,\n",
    "        'device': device\n",
    "    },\n",
    "    agent_name_list[1]:{\n",
    "        'name': agent_name_list[1],\n",
    "        'environment': None,\n",
    "        'reward_function': risk_reward,\n",
    "        'reward_params': {'n':5},\n",
    "        'env_state_mod_func': env_mod_func,\n",
    "        'input_size': 11,\n",
    "        'hidden_size': 256,\n",
    "        'output_size':3,\n",
    "        'activation_function': nn.LeakyReLU(),\n",
    "        'num_hidden_layers': 2,\n",
    "        'buffer_size': 150,\n",
    "        'batch_size': 30,\n",
    "        'alpha': 0.005,\n",
    "        'gamma':0.97,\n",
    "        'opt_wgt_dcy': 0.01,\n",
    "        'dropout_rate': 0.25,\n",
    "        'device': device\n",
    "    },\n",
    "        agent_name_list[2]:{\n",
    "        'name': agent_name_list[2],\n",
    "        'environment': None,\n",
    "        'reward_function': zero_reward,\n",
    "        'reward_params': {},\n",
    "        }}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_dic = {}\n",
    "\n",
    "for agent_name, agent_class in agent_classes.items():\n",
    "            selected_agent = agent_class(**agent_params[agent_name])\n",
    "            agents_dic[agent_name] = selected_agent\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Inputs\n",
    "training_range = ('2007-01-01','2010-12-31')\n",
    "trn_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in training_range]\n",
    "\n",
    "# Validation Inputs\n",
    "validation_range = ('2011-01-01', '2011-12-31')\n",
    "val_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in validation_range]\n",
    "\n",
    "# Testing Inputs\n",
    "testing_range = ('2011-01-01', '2013-12-31')\n",
    "tst_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in testing_range]\n",
    "\n",
    "# Scaling Inputs\n",
    "scaling_range = ('2007-01-01','2010-12-31')\n",
    "scale_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in scaling_range]\n",
    "\n",
    "\n",
    "trn_idx = {}\n",
    "val_idx = {}\n",
    "tst_idx = {}\n",
    "scale_idx = {}\n",
    "\n",
    "for stock, file in stock_inputs.items():\n",
    "    if stock in set(trn_keys + val_keys + tst_keys):\n",
    "        if stock in trn_keys:\n",
    "            trn_idx[stock] = env_data[stock].gen_rw_idxs(trn_dt_range)\n",
    "        if stock in val_keys:\n",
    "            val_idx[stock] = env_data[stock].gen_rw_idxs(val_dt_range)\n",
    "        if stock in tst_keys:\n",
    "            tst_idx[stock] = env_data[stock].gen_rw_idxs(tst_dt_range)\n",
    "        \n",
    "        scale_idx[stock] = env_data[stock].gen_idxs(scale_dt_range)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {'training_episodes': 15,\n",
    "                   'epsilon_decya_func': linear_decay,\n",
    "                   'initial_epsilon': 0.9,\n",
    "                   'final_epsilon': 0.1,\n",
    "                   'update_q_freq': 2,\n",
    "                   'update_tgt_freq': 8,\n",
    "                   'save_path': export_path,\n",
    "                   'metric_func': metric_function,\n",
    "                   'min_training_episodes': 5,\n",
    "                   'early_stop': True,\n",
    "                   'stop_metric': 'val_tot_r',\n",
    "                   'stop_patience': 3,\n",
    "                   'stop_delta': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_agent:\n",
    "    filtered_agents = {\n",
    "        decision_agent: agents_in_setup\n",
    "        for decision_agent, agents_in_setup in agent_setup.items()\n",
    "        if decision_agent in agents_to_train\n",
    "    }\n",
    "\n",
    "    for decision_agent, agents_in_setup in filtered_agents.items():\n",
    "        for key in trn_keys:\n",
    "            rl_env = env[key]\n",
    "            \n",
    "        \n",
    "            # Using Csvs\n",
    "            if import_tn_csvs:\n",
    "                timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')\n",
    "            \n",
    "            if not timesnet.window_scaling:\n",
    "                start_idx = scale_idx[key][0]\n",
    "                end_idx = scale_idx[key][1]\n",
    "                timesnet.fit_standardscaler(env_data[key]['raw_env'][start_idx:end_idx,:])\n",
    "            \n",
    "            # Setup agents with environment  \n",
    "            for agent in agents_in_setup:\n",
    "                rl_env.add_agent(agent)\n",
    "                agents_dic[agent].set_environment(rl_env)\n",
    "            rl_env.set_decision_agent(decision_agent)\n",
    "            \n",
    "            # Train Sub-subagents\n",
    "            for agent in agents_in_setup:\n",
    "                if agent is not decision_agent:\n",
    "                    agents_dic[agent].train(start_idx=trn_idx[key][0],\n",
    "                                            end_idx=trn_idx[key][1],\n",
    "                                            val_start_idx= val_idx[key][0],\n",
    "                                            val_end_idx=val_idx[key][1],                                    \n",
    "                                            **training_params)\n",
    "                    # Save Agent\n",
    "                    save_agent_path = save_path_root + f'/{agent}/'\n",
    "                    os.makedirs(save_path_root, exist_ok=True)\n",
    "                    agents_dic[agent].export_Q_nn(save_agent_path + agent)\n",
    "                            \n",
    "            # Train Decision Agent\n",
    "            agents_dic[decision_agent].train(start_idx=trn_idx[key][0],\n",
    "                                        end_idx=trn_idx[key][1],\n",
    "                                        val_start_idx= val_idx[key][0],\n",
    "                                        val_end_idx=val_idx[key][1],                                    \n",
    "                                        **training_params)\n",
    "            # Save Agent\n",
    "            save_agent_path = save_path_root + f'/{decision_agent}_{key}_TRN_{trn_idx[key][0]}-{trn_idx[key][1]}/'\n",
    "            os.makedirs(save_agent_path, exist_ok=True)\n",
    "            agents_dic[decision_agent].export_Q_nn(save_agent_path + decision_agent)\n",
    "            env_data_record = rl_env.get_step_data()\n",
    "            agent_data_record = agents_dic[decision_agent].get_step_data()\n",
    "            training_record =  pd.concat([env_data_record, agent_data_record], axis=1, join='inner')\n",
    "            training_record.to_csv(f'{save_agent_path}step_data.csv')\n",
    "    \n",
    "            # Remove Agent\n",
    "            for agent in agents_in_setup:\n",
    "                rl_env.remove_agent(agent)\n",
    "                agents_dic[agent].set_environment(None)\n",
    "        \"\"\"\n",
    "        ## Export Training Session Data to CSV\n",
    "        ddqn_trn = best_ddqn_agent.get_training_episodic_data()\n",
    "        ddqn_trn.to_csv('test.csv')\n",
    "        display(ddqn_trn)\n",
    "        env[key].remove_agent(best_ddqn_agent.get_name())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_params = {DdqnAgent: {\n",
    "                   'metric_func': metric_function,\n",
    "                   'metric_func_arg': {},\n",
    "                   'testing_episodes':1},\n",
    "                  RandomAgent: {\n",
    "                    'metric_func': metric_function,\n",
    "                   'metric_func_arg': {},\n",
    "                   'testing_episodes':100}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_agent:\n",
    "    result_dic_struct = ['stock','agent','test_interval','test_num']\n",
    "    results = {}\n",
    "\n",
    "    for key in tst_keys:\n",
    "        \n",
    "        # Init Record[Stock]\n",
    "        results[key] = {}\n",
    "        test_key = f'{tst_idx[key][0]}:{tst_idx[key][1]}'\n",
    "        stock_price_data = env_data[key]['rw_raw_price_env'][tst_idx[key][0]:tst_idx[key][1],-1,0]\n",
    "        rl_env = env[key]\n",
    "\n",
    "        for decision_agent, agents_in_setup in agent_setup.items():\n",
    "\n",
    "            # Init Record[Stock][Agent]\n",
    "            results[key][decision_agent] = {}\n",
    "            \n",
    "            # Init Record[Stock][Agent][test_interval]\n",
    "            results[key][decision_agent][test_key] = {}   # Different Test Keys will need loop\n",
    "            \n",
    "            # Using Csvs       \n",
    "            if import_tn_csvs:\n",
    "                timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')\n",
    "            \n",
    "            #\n",
    "            if not timesnet.window_scaling:\n",
    "                start_idx = scale_idx[key][0]\n",
    "                end_idx = scale_idx[key][1]\n",
    "                timesnet.fit_standardscaler(env_data[key]['raw_env'][start_idx:end_idx,:])\n",
    "            \n",
    "            # Setup agents with environment  \n",
    "            for agent in set([decision_agent] + agents_in_setup):\n",
    "                rl_env.add_agent(agent)\n",
    "                agents_dic[agent].set_environment(rl_env)\n",
    "            rl_env.set_decision_agent(decision_agent)\n",
    "            \n",
    "            # Enable Randomess if Agent is of class RandomAgent\n",
    "            if isinstance(agents_dic[decision_agent], RandomAgent):\n",
    "                new_random_seed = random.randint(1, 10**9)\n",
    "                set_seed(new_random_seed)\n",
    "                \n",
    "            # Test Decision Agent\n",
    "            params = testing_params[agent_classes[decision_agent]]\n",
    "            agents_dic[decision_agent].test(start_idx=tst_idx[key][0],\n",
    "                                            end_idx=tst_idx[key][1],\n",
    "                                            **params)\n",
    "            test_results = agents_dic[decision_agent].get_testing_episodic_data()\n",
    "            trade_actions_per_test = test_results['tst_actions']\n",
    "                \n",
    "            for idx, action_set in enumerate(trade_actions_per_test):\n",
    "                test_metrics = agentperform.agent_stock_performance(stock_price_ts=np.array(stock_price_data),\n",
    "                                                                    trade_ts=np.array(action_set),\n",
    "                                                                    stock_name=key,\n",
    "                                                                    agent_name=decision_agent,\n",
    "                                                                    display_graph=False, \n",
    "                                                                    save_graphic= False,\n",
    "                                                                    path_file = None)\n",
    "                del test_metrics['stock']\n",
    "                del test_metrics['agent_name']                                                                     \n",
    "                results[key][decision_agent][test_key][idx] = test_metrics\n",
    "            \n",
    "            # Remove Agent\n",
    "            for agent in set([decision_agent] + agents_in_setup):\n",
    "                rl_env.remove_agent(agent)\n",
    "                agents_dic[agent].set_environment(None)\n",
    "\n",
    "    display(results)\n",
    "    set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggreating Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_agent:\n",
    "    aggerate_results = {}\n",
    "    for agent in agent_name_list:\n",
    "        aggerate_results[agent] = {}\n",
    "        for stock in tst_keys:\n",
    "            aggerate_results[agent][stock] = {}\n",
    "            test_key = f'{tst_idx[stock][0]}:{tst_idx[stock][1]}'\n",
    "            aggerate_results[agent][stock][test_key] = {}\n",
    "            values = np.empty((0,len(metrics)))\n",
    "            for test_num in range(testing_params[agent_classes[agent]]['testing_episodes']):\n",
    "\n",
    "                values_array = [results[stock][agent][test_key][test_num][key] for key in metrics]\n",
    "                current_values = np.array(values_array)\n",
    "                values = np.vstack((values,current_values))\n",
    "\n",
    "                means_for_metrics = np.mean(values, axis=0)\n",
    "                std_for_metrics = np.std(values, axis=0)\n",
    "            \n",
    "            for idx,metric in enumerate(metrics):\n",
    "                aggerate_results[agent][stock][test_key][metric] = (means_for_metrics[idx],std_for_metrics[idx])\n",
    "\n",
    "    summarized_aggerate_results = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        model_list = []\n",
    "        dataset_name = []\n",
    "        scores = []\n",
    "        for agent in aggerate_results.keys():\n",
    "            \n",
    "            model_list.append(agent)\n",
    "            score_list = []\n",
    "            for stock in aggerate_results[agent].keys():\n",
    "                for test in aggerate_results[agent][stock].keys():\n",
    "                    run_name = stock + \"-\" + test\n",
    "                    if run_name not in dataset_name:\n",
    "                        dataset_name.append(run_name)\n",
    "                    score = aggerate_results[agent][stock][test][metric][0]\n",
    "                    score_list.append(np.round(score,2))\n",
    "            scores.append(score_list)\n",
    "\n",
    "        score_array = np.array(scores).T\n",
    "\n",
    "        df = pd.DataFrame(score_array,columns=model_list)\n",
    "        df['dataset'] = dataset_name\n",
    "\n",
    "        column_order = ['dataset'] + [col for col in df.columns if col != 'dataset']\n",
    "        df = df[column_order]\n",
    "\n",
    "        model_means = list(zip(model_list,df[model_list].mean()))\n",
    "\n",
    "        summarized_aggerate_results[metric] = (df, model_means)\n",
    "\n",
    "\n",
    "    display(summarized_aggerate_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_agent:\n",
    "    for metric in metrics:\n",
    "        display(metric)\n",
    "        display(summarized_aggerate_results[metric][0])\n",
    "        test = prob_evaluate.generate_rank_array_from_dataframe(summarized_aggerate_results[metric][0],\n",
    "                                                                model_list,equal_rank_behav=\"mean\",\n",
    "                                                                rank_order=aval_metrics_rank_dic[metric])\n",
    "        display(test)\n",
    "        stat, critical_f_value, reject_null_hypo = prob_evaluate.iman_davenport_test(test,0.95,arr_order='cols')\n",
    "        display(stat, critical_f_value, reject_null_hypo)\n",
    "\n",
    "        results1 = prob_evaluate.nemenyi_test(test,0.95,model_list)\n",
    "        display(results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(env_data['DJI']['raw_env'][0:5])\n",
    "display(env_data['DJI']['raw_df'].head())\n",
    "display(env_data['DJI']['raw_price_env'][0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(scale_dt_range,scale_idx)\n",
    "#display(env_data['DJI']['raw_df'].iloc[1007])\n",
    "#display(agents_dic['profit'].get_step_data())\n",
    "#isplay(env['AAPL'].get_step_data())\n",
    "\n",
    "test_scalers = {}\n",
    "for stock in tst_keys:\n",
    "    test_scalers[stock] = []\n",
    "    for idx, col in enumerate(columns):\n",
    "        start_idx = scale_idx[stock][0]\n",
    "        end_idx = scale_idx[stock][1]\n",
    "        new_scaler = preprocessing.StandardScaler()\n",
    "        display(env_data[key]['raw_env'][start_idx:end_idx,idx])\n",
    "        #new_scaler.fit(env_data[key]['raw_env'][start_idx:end_idx,:])\n",
    "        #test_scalers[stock].append(new_scaler)\n",
    "\n",
    "example = (np.array([[1.157743e+04, 1.171147e+04, 1.157735e+04, 1.167075e+04,2.034200e+08],\n",
    "       [1.167090e+04, 1.169822e+04, 1.163574e+04, 1.169118e+04, 1.786300e+08],\n",
    "       [1.168861e+04, 1.174268e+04, 1.165289e+04, 1.172289e+04, 1.699900e+08],\n",
    "       [1.171693e+04, 1.173674e+04, 1.166746e+04, 1.169731e+04, 1.930800e+08],\n",
    "       [1.169686e+04, 1.172694e+04, 1.159968e+04, 1.167476e+04, 1.887200e+08],\n",
    "       [1.167234e+04, 1.167733e+04, 1.157387e+04, 1.163745e+04, 1.503400e+08],\n",
    "       [1.163851e+04, 1.170412e+04, 1.163548e+04, 1.167188e+04, 1.574400e+08],\n",
    "       [1.167362e+04, 1.178223e+04, 1.167362e+04, 1.175544e+04, 1.449600e+08],\n",
    "       [1.175370e+04, 1.175725e+04, 1.170053e+04, 1.173190e+04, 1.616600e+08],\n",
    "       [1.173213e+04, 1.179415e+04, 1.169883e+04, 1.178738e+04, 2.007700e+08],\n",
    "       [1.178382e+04, 1.185878e+04, 1.177799e+04, 1.183793e+04, 2.033900e+08],\n",
    "       [1.183421e+04, 1.186124e+04, 1.179846e+04, 1.182529e+04, 1.662500e+08],\n",
    "       [1.182370e+04, 1.184516e+04, 1.174477e+04, 1.182280e+04, 1.808000e+08],\n",
    "       [1.182295e+04, 1.190548e+04, 1.182280e+04, 1.187184e+04, 2.494800e+08],\n",
    "       [1.187343e+04, 1.198294e+04, 1.186798e+04, 1.198052e+04, 1.840000e+08],\n",
    "       [1.198052e+04, 1.198597e+04, 1.189874e+04, 1.197719e+04, 1.919500e+08],\n",
    "       [1.197885e+04, 1.202052e+04, 1.196183e+04, 1.198544e+04, 1.683200e+08],\n",
    "       [1.198536e+04, 1.201953e+04, 1.197193e+04, 1.198983e+04, 1.677700e+08],\n",
    "       [1.199036e+04, 1.201242e+04, 1.180304e+04, 1.182370e+04, 2.141700e+08],\n",
    "       [1.182439e+04, 1.189193e+04, 1.181788e+04, 1.189193e+04, 2.065800e+08],\n",
    "       [1.189250e+04, 1.205075e+04, 1.189250e+04, 1.204016e+04, 1.808900e+08],\n",
    "       [1.203827e+04, 1.205791e+04, 1.201851e+04, 1.204197e+04, 1.434400e+08],\n",
    "       [1.204068e+04, 1.208054e+04, 1.198105e+04, 1.206226e+04, 1.437100e+08],\n",
    "       [1.206173e+04, 1.209242e+04, 1.202578e+04, 1.209215e+04, 1.217800e+08],\n",
    "       [1.209238e+04, 1.218876e+04, 1.209230e+04, 1.216163e+04, 1.329600e+08],\n",
    "       [1.215270e+04, 1.223879e+04, 1.215005e+04, 1.223315e+04, 1.266500e+08],\n",
    "       [1.222929e+04, 1.225423e+04, 1.218819e+04, 1.223989e+04, 1.629100e+08],\n",
    "       [1.223966e+04, 1.223966e+04, 1.215694e+04, 1.222929e+04, 2.744400e+08]]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MADDQN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
