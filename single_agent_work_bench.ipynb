{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atpmeCKJQFm_"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAeE9oqlQFnD"
      },
      "source": [
        "## Google Colab Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C68ncf6KQFnE"
      },
      "source": [
        "### Install Python Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-8QZlwoQFnE",
        "outputId": "c93ef470-6de0-43da-f16a-7b75e1717e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in another environment (e.g., VS Code)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Code specific to Google Colab\n",
        "    print(\"Running in Google Colab\")\n",
        "\n",
        "    # Additional setup commands for Colab\n",
        "    !pip install neuralforecast\n",
        "    !pip install gymnasium\n",
        "    !pip install QuantStats\n",
        "else:\n",
        "    # Code for other environments (e.g., VS Code)\n",
        "    print(\"Running in another environment (e.g., VS Code)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s8-HY3ZQFnG"
      },
      "source": [
        "### Install RL Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT3SpOINQFnH",
        "outputId": "ff9fd1b4-d9dc-471a-a5cb-0119575dec34"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    # Retrive required files\n",
        "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/environments/stockenv.py\n",
        "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/cleandata.py\n",
        "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/data.py\n",
        "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/epsilon_decay.py\n",
        "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/agentperform.py\n",
        "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/prob_evaluate.py\n",
        "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/ddqn.py\n",
        "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/random.py\n",
        "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/baseagent.py\n",
        "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/rewards/stockmarket.py\n",
        "    # Move all directories and files from content/raw.githubusercontent.com to content/\n",
        "    !mv /content/raw.githubusercontent.com/* /content/\n",
        "\n",
        "    # Delete the raw.githubusercontent.com directory\n",
        "    !rm -rf /content/raw.githubusercontent.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrbfXX7DQFnI"
      },
      "source": [
        "# Activate Python Libraries & Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKyWuV4wQFnI",
        "outputId": "324836b6-7087-48a0-d20a-823894e3aaa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA (GPU support) is not available. PyTorch is running on CPU.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import optuna\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import utilities.agentperform as agentperform\n",
        "import utilities.cleandata as cln\n",
        "from utilities.epsilon_decay import linear_decay\n",
        "from utilities.data import UniStockEnvDataStruct, TimesNetProcessing, ModifyDDQNAgentState\n",
        "from utilities import prob_evaluate\n",
        "from agents.ddqn import DdqnAgent\n",
        "from agents.random import RandomAgent\n",
        "from rewards.stockmarket import future_profit, risk_reward, zero_reward\n",
        "from environments.stockenv import ContinuousOHLCVEnv\n",
        "from datetime import datetime\n",
        "from neuralforecast.core import NeuralForecast\n",
        "from neuralforecast.models import TimesNet\n",
        "from neuralforecast.losses.numpy import mae, mse\n",
        "import logging\n",
        "from sklearn import preprocessing\n",
        "\n",
        "#\n",
        "logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").addHandler(logging.NullHandler())\n",
        "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").addHandler(logging.NullHandler())\n",
        "os.environ['NIXTLA_ID_AS_COL'] = '1' # Prevent Warning\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    # Python random module\n",
        "    random.seed(seed)\n",
        "\n",
        "    # NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # If you are using CUDA\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "        # Additional settings to force determinism in your operations:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "if torch.cuda.is_available():\n",
        "    # Get the current device\n",
        "    device = torch.cuda.current_device()\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print(\"CUDA (GPU support) is not available. PyTorch is running on CPU.\")\n",
        "\n",
        "\n",
        "def decimal_to_text(decimal_number):\n",
        "    # Remove the decimal point and convert to integer\n",
        "    integer_part = int(decimal_number * 1000)\n",
        "    # Convert the integer to text\n",
        "    text_representation = str(integer_part)\n",
        "    return text_representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSGPXNdIQFnK"
      },
      "source": [
        "# RL Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqgSqhMDQFnK"
      },
      "source": [
        "## Parameters & CSV Locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_iIDvcGSQFnK"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "set_seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "\n",
        "if not IN_COLAB:\n",
        "    pwd = \"C:/programming/MADDQN\"\n",
        "    sys.path.append(pwd)\n",
        "\n",
        "    # Output Path Location for CSV export\n",
        "    export_path = pwd + \"/output_data/test/\"\n",
        "else:\n",
        "    pwd = '/content'\n",
        "    export_path = pwd + \"/output_data/test/\"\n",
        "\n",
        "# Input Data Location, File Name, Stock name for labels\n",
        "input_url = 'https://raw.githubusercontent.com/CodeBeckZero/MADDQN/main/input_data'\n",
        "\n",
        "stock_inputs ={'DJI':'^DJI_daily.csv',\n",
        "               'NDAQ': '^IXIC_daily.csv',\n",
        "               'SP500': '^SPX_daily.csv',\n",
        "               'AAPL': 'AAPL_daily.csv',\n",
        "               'AMZN': 'AMZN_daily.csv',\n",
        "               'GOOGL': 'GOOGL_daily.csv',\n",
        "               'MSFT': 'MSFT_daily.csv',\n",
        "               'SINE': 'sine_wave_daily.csv',\n",
        "               'FORD': 'F_daily.csv',\n",
        "               'JNJ': 'JNJ_daily.csv',\n",
        "               'NEE': 'NEE_daily.csv',\n",
        "               'PFE': 'PFE_daily.csv',\n",
        "               'TSLA': 'TSLA_daily.csv',\n",
        "               'COKE': 'COKE_daily.csv',\n",
        "               'PG': 'PG_daily.csv'}\n",
        "\n",
        "# Training Inputs\n",
        "trn_keys = ['DJI','NDAQ','SP500']\n",
        "training_range = ('2007-01-01','2019-12-31')\n",
        "trn_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in training_range]\n",
        "\n",
        "# Validation Inputs\n",
        "validate = False\n",
        "val_keys = trn_keys\n",
        "validation_range = ('2020-01-01', '2021-12-31')\n",
        "val_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in validation_range]\n",
        "\n",
        "# Testing Inputs\n",
        "tst_keys = ['AAPL','AMZN','GOOGL','MSFT','FORD','JNJ','NEE','PFE','TSLA','COKE','PG']\n",
        "testing_range = ('2020-01-01', '2023-12-31')\n",
        "tst_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in testing_range]\n",
        "\n",
        "# Environmental Inputs\n",
        "window_size = 28 # Needs to match the size Timesnet is trained on\n",
        "price_based_on = 'close'\n",
        "columns = ['open','high','low','close','volume']\n",
        "\n",
        "\n",
        "# Metrics Interested in\n",
        "metrics = ['n_trades','n_wins', 'win_percentage','cumulative_return','sortino','max_drawdown','sharpe', 'trade_dur_avg']\n",
        "\n",
        "# Ranking type for Metrics\n",
        "aval_metrics_rank_dic = {'n_trades':'max','n_wins': 'max' ,'n_losses':'max','win_percentage':'max','cumulative_return':'max',\n",
        "                 'sortino':'max','max_drawdown':'min', 'sharpe':'max', 'trade_dur_avg':'max', 'trade_dur_min':'max',\n",
        "                 'trade_dur_max':'max','buy_hold':'max'}\n",
        "## See agentperform.py -> results dictionary for options\n",
        "\n",
        "# Modifying enviornment state before the\n",
        "env_mod_parms = {'columns': columns, 'scaling_type': 'col', 'scaler_func':preprocessing.StandardScaler()}\n",
        "scaling_range = ('2007-01-01','2020-12-31')\n",
        "scale_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in scaling_range]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KjIdWWtQFnL"
      },
      "source": [
        "## RL Enviornment Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qDmzkFh8QFnM"
      },
      "outputs": [],
      "source": [
        "env_data = {}\n",
        "env = {}\n",
        "\n",
        "\n",
        "for stock, file in stock_inputs.items():\n",
        "    if stock in set(trn_keys + val_keys + tst_keys):\n",
        "        # Import\n",
        "        df = cln.YAHOO_csv_input(file, input_url)\n",
        "        data_dic = UniStockEnvDataStruct(df,columns,price_based_on,window_size)\n",
        "        env_data[stock] = data_dic\n",
        "        env[stock] = ContinuousOHLCVEnv(name=stock,\n",
        "                                        ohlcv_data = env_data[stock]['rw_raw_env'] ,\n",
        "                                        stock_price_data= env_data[stock]['rw_raw_price_env'],\n",
        "                                        commission_rate=0.005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nOEPsX6QFnM"
      },
      "source": [
        "## Generating Indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v654qbK7QFnM"
      },
      "outputs": [],
      "source": [
        "trn_idx = {}\n",
        "val_idx = {}\n",
        "tst_idx = {}\n",
        "scale_idx = {}\n",
        "\n",
        "for stock, file in stock_inputs.items():\n",
        "    if stock in set(trn_keys + val_keys + tst_keys):\n",
        "        if stock in trn_keys:\n",
        "            trn_idx[stock] = env_data[stock].gen_rw_idxs(trn_dt_range)\n",
        "        if stock in val_keys:\n",
        "            val_idx[stock] = env_data[stock].gen_rw_idxs(val_dt_range)\n",
        "        if stock in tst_keys:\n",
        "            tst_idx[stock] = env_data[stock].gen_rw_idxs(tst_dt_range)\n",
        "\n",
        "        scale_idx[stock] = env_data[stock].gen_idxs(scale_dt_range)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2RR5eb9QFnN"
      },
      "source": [
        "# Workbench Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAmGyMmuQFnN"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yibhlUW2QFnN"
      },
      "outputs": [],
      "source": [
        "## Create Directory\n",
        "case_name = '/test008'\n",
        "save_path_root = pwd + case_name\n",
        "os.makedirs(save_path_root, exist_ok=False)\n",
        "\n",
        "# Timesnet\n",
        "## Number of price predictions in the Future by TimesNet (Required for RL agent's input layer)\n",
        "n_prediction = 5\n",
        "## Need to Train TimesNet Preprocessing model (processed every cycle)\n",
        "train_tn_model = False\n",
        "## Importing TimesNet Preprocessing model (processed every cycle)\n",
        "import_tn_model = False\n",
        "tn_model_path = pwd + '/gen_data/timesnet/'\n",
        "## Use Imported CSVs from Preprocessing model (no processing, straight to RL agent)\n",
        "import_tn_csvs = True\n",
        "tn_csvs_path = pwd + '/gen_data/csvs/'\n",
        "## No modifaction of environmental state before input to agent\n",
        "no_tn_preprocessing = False\n",
        "\n",
        "# Limited Exploratory Hyperparmater Discover for single RL Agent\n",
        "hyperparam_discovery = False\n",
        "\n",
        "# Traditional Training/Testing\n",
        "## Train Agent(s)\n",
        "train_agent = True\n",
        "# Test Agent(s)\n",
        "test_agent = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SViA7zL9QFnN"
      },
      "outputs": [],
      "source": [
        "if  not(train_tn_model ^ import_tn_model ^ import_tn_csvs ^ no_tn_preprocessing):\n",
        "    raise ValueError(\"Only one preprocessing options can and must be selected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU5O6X0oQFnO"
      },
      "source": [
        "## Metric Function\n",
        "\n",
        "Function that generates metric from enviornment that will be used during validation phase of training or testing phase of model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GzYRJ4-dQFnO"
      },
      "outputs": [],
      "source": [
        "def metric_function(env):\n",
        "    metric = env.step_info[-1]['New Portfolio Value'] -  env.step_info[-1]['Portfolio Value']\n",
        "    return metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1_EYhgoQFnP"
      },
      "source": [
        "# TimesNet Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7OpXBjUQFnP"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckS9XQTxQFnP"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nMjZ8cZJQFnQ"
      },
      "outputs": [],
      "source": [
        "if train_tn_model:\n",
        "\n",
        "    model = TimesNet(h = n_prediction,          # Forecast horizon\n",
        "                    input_size = window_size,   # Length of Batches\n",
        "                    batch_size = 1,             # Number of timeseries to predict\n",
        "                    #futr_exog_list = remaining_columns,\n",
        "                    hidden_size = 128,          # Size of embedding for embedding and encoders,\n",
        "                    dropout = 0.40,             # Dropout for embeddings\n",
        "                    conv_hidden_size = 3,       # Channels for the inception block\n",
        "                    top_k = 5,                  # Top num of periods from FFT considered\n",
        "                    num_kernels = 13,           # number of kernels for the inception block\n",
        "                    encoder_layers = 3,         # num of encoders\n",
        "                    max_steps = 1000,           # of training steps\n",
        "                    early_stop_patience_steps = 10, #early stoppage on validation\n",
        "                    val_check_steps = 100,      # Val check every X steps,\n",
        "                    windows_batch_size = 150,   # Number of windows in training epoch,\n",
        "                    num_workers_loader = 7,\n",
        "                    learning_rate = 0.0003,\n",
        "                    random_seed = RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKX00xoOQFnQ"
      },
      "source": [
        "### Code Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RRjzLmp2QFnR"
      },
      "outputs": [],
      "source": [
        "if train_tn_model:\n",
        "  nf = NeuralForecast(models=[model], freq='d')\n",
        "  results = {}\n",
        "  for key in trn_keys:\n",
        "    results[key] = nf.fit(df=env[key],val_size=0.2)\n",
        "\n",
        "  nf.save(path= tn_model_path,\n",
        "          model_index=None,\n",
        "          overwrite=True,\n",
        "          save_dataset=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JHWsNYIQFnR"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YCtZ140VQFnS"
      },
      "outputs": [],
      "source": [
        "if import_tn_model:\n",
        "# Define the correct path\n",
        "  if IN_COLAB:\n",
        "\n",
        "    model_path = os.path.join(os.getcwd(), 'gen_data', 'timesnet')\n",
        "\n",
        "    # Ensure the directory and file exist\n",
        "    if os.path.exists(model_path):\n",
        "        nf = NeuralForecast.load(path=model_path)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Model path {model_path} does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI00cUAeQFnS"
      },
      "source": [
        "## CSV Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iVuHiiPNQFnS"
      },
      "outputs": [],
      "source": [
        "if import_tn_csvs:\n",
        "    env_mod_func_dic = {}\n",
        "    if IN_COLAB:\n",
        "        # Input Data Location, File Name, Stock name for labels\n",
        "        csv_path = 'https://raw.githubusercontent.com/CodeBeckZero/MADDQN/main/gen_data/csvs/'\n",
        "\n",
        "    else:\n",
        "        csv_path  = tn_csvs_path\n",
        "\n",
        "    stock_tn ={'DJI':'DJI_tn.csv',\n",
        "                'NDAQ': 'NDAQ_tn.csv',\n",
        "                'SP500': 'SP500_tn.csv',\n",
        "                'AAPL': 'AAPL_tn.csv',\n",
        "                'AMZN': 'AMZN_tn.csv',\n",
        "                'GOOGL': 'GOOGL_tn.csv',\n",
        "                'MSFT': 'MSFT_tn.csv',\n",
        "                'FORD': 'FORD_tn.csv',\n",
        "                'JNJ': 'JNJ_tn.csv',\n",
        "                'NEE': 'NEE_tn.csv',\n",
        "                'PFE': 'PFE_tn.csv',\n",
        "                'TSLA': 'TSLA_tn.csv',\n",
        "                'COKE': 'COKE_tn.csv',\n",
        "                'PG': 'PG_tn.csv'}\n",
        "\n",
        "    for stock in set(trn_keys + val_keys + tst_keys):\n",
        "        import_csv_path = f'{csv_path}/{stock_tn[stock]}'\n",
        "        temp_env_mod_fuc = ModifyDDQNAgentState(uni_data=env_data[stock],\n",
        "                                                columns=columns,\n",
        "                                                csv_import=import_tn_csvs,\n",
        "                                                csv_path=import_csv_path,\n",
        "                                                scaling_type='col',\n",
        "                                                scaler_func=preprocessing.StandardScaler(),\n",
        "                                                start_scale_idx=scale_idx[stock][0],\n",
        "                                                finish_scale_idx=scale_idx[stock][1])\n",
        "        env_mod_func_dic[stock] = temp_env_mod_fuc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuCCAyfQQFnT"
      },
      "source": [
        "## Direct\n",
        "Current direct default is to take the rolling windows last entry and convert it to a list prior to input to agent. Agent's experience memory is currently list-based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "13TlC9uQQFnT"
      },
      "outputs": [],
      "source": [
        "if no_tn_preprocessing:\n",
        "    env_mod_func_dic = {}\n",
        "    for stock in set(trn_keys + val_keys + tst_keys):\n",
        "        temp_env_mod_fuc = ModifyDDQNAgentState(uni_data=env_data[stock],\n",
        "                                                columns=columns,\n",
        "                                                csv_import= None,\n",
        "                                                csv_path= None,\n",
        "                                                scaling_type= None,\n",
        "                                                scaler_func=None)\n",
        "        env_mod_func_dic[stock] = temp_env_mod_fuc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90L1szCuQFnU"
      },
      "source": [
        "# Exploratory Hyperparameterization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WVVfJu8QFnU"
      },
      "source": [
        "## Interval Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TF9AGtlpQFnV"
      },
      "outputs": [],
      "source": [
        "if hyperparam_discovery:\n",
        "    # Training Inputs\n",
        "    hyp_training_range = ('2007-01-01','2020-12-31')\n",
        "    hyp_trn_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_training_range]\n",
        "\n",
        "    # Validation Inputs\n",
        "    hyp_validation_range = ('2021-01-01', '2021-12-31')\n",
        "    hyp_val_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_validation_range]\n",
        "\n",
        "    # Testing Inputs\n",
        "    hyp_testing_range = ('2021-01-01', '2023-12-31')\n",
        "    hyp_tst_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_testing_range]\n",
        "\n",
        "    hyp_trn_idx = {}\n",
        "    hyp_val_idx = {}\n",
        "    hyp_tst_idx = {}\n",
        "\n",
        "    for stock, file in stock_inputs.items():\n",
        "        if stock in set(trn_keys + val_keys + tst_keys):\n",
        "            if stock in trn_keys:\n",
        "                hyp_trn_idx[stock] = env_data[stock].gen_rw_idxs(hyp_trn_dt_range)\n",
        "            if stock in val_keys:\n",
        "                hyp_val_idx[stock] = env_data[stock].gen_rw_idxs(hyp_val_dt_range)\n",
        "            if stock in tst_keys:\n",
        "                hyp_tst_idx[stock] = env_data[stock].gen_rw_idxs(hyp_tst_dt_range)\n",
        "\n",
        "    display(hyp_trn_idx,hyp_val_idx,hyp_tst_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOgs_k5QFnV"
      },
      "source": [
        "## Parameter Search & Code Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_KBlSkuFQFnW"
      },
      "outputs": [],
      "source": [
        "if hyperparam_discovery:\n",
        "\n",
        "    # For Objective function, need to create agent name before to link agent with enviornment\n",
        "    agent_name = 'hyp_discovery_agent'\n",
        "    agent_path = export_path + '/' + agent_name\n",
        "    metric = 'val_tot_r'\n",
        "    max_len_buf = np.round(hyp_trn_idx['DJI'][1] - hyp_trn_idx['DJI'][0] + window_size, -2) -10 # manual input, could be error here if\n",
        "    print(f'Max Mem Length: {max_len_buf}')\n",
        "\n",
        "    def objective(trial):\n",
        "\n",
        "        # Define the hyperparameters to search over\n",
        "\n",
        "        ## NN hyperparameters\n",
        "        sug_hidden_layers = trial.suggest_int('hidden_layers', low=1, high=3)\n",
        "        sug_hidden_size = trial.suggest_int('hidden_size', low=64, high=512, step=64)\n",
        "        sug_update_q_freq = trial.suggest_int('update_q_freq',low=1,high=5)\n",
        "        sug_update_tgt_freq = trial.suggest_int('update_tgt_freq',low=5,high=15)\n",
        "\n",
        "        ## Activation Function Passing\n",
        "        activation_functions = {\n",
        "        'LRELUd': nn.LeakyReLU(),\n",
        "        'LRELUs02': nn.LeakyReLU(negative_slope=0.2),\n",
        "        'GELU': nn.GELU(),\n",
        "        'TANH': nn.Tanh(),\n",
        "        'SELU':nn.SELU(),\n",
        "        'SILU': nn.SiLU()\n",
        "        }\n",
        "        sug_activation_function_name = trial.suggest_categorical('activation_function', list(activation_functions.keys()))\n",
        "        sug_activation_function = activation_functions[sug_activation_function_name]\n",
        "\n",
        "        \"\"\"\n",
        "        ## Reward Function Passing\n",
        "        reward_functions = {\n",
        "        'profit': future_profit(None,5),\n",
        "        'risk': risk_reward(None,5),\n",
        "        }\n",
        "        sug_reward_function_name = trial.suggest_categorical('reward_function', list(reward_functions.keys()))\n",
        "        sug_reward_function = reward_functions[sug_reward_function_name]\n",
        "        \"\"\"\n",
        "        ## Optimizer hyperparameters\n",
        "        sug_opt_lre = trial.suggest_categorical('opt_lre',[0.0001,0.0005,0.001, 0.005, 0.01, 0.05, 0.1])\n",
        "        sug_gamma = trial.suggest_float('gamma',low=0.90,high=0.99,step=0.01)\n",
        "        ## Memory Replay hyperparameters\n",
        "        sug_buffer_size = trial.suggest_int('buffer_size',low=100,high=max_len_buf,step=10)\n",
        "        sug_batch_size = trial.suggest_int('batch_size',low=10,high=sug_buffer_size,step=5)\n",
        "\n",
        "        # Saving Setup\n",
        "        ## Current Parameter Values:\n",
        "        cur_n_fcl = trial.params['hidden_layers']\n",
        "        cur_fcl_size = trial.params['hidden_size']\n",
        "        cur_q_freq = trial.params['update_q_freq']\n",
        "        cur_tgt_freq = trial.params['update_tgt_freq']\n",
        "        cur_act_func = trial.params['activation_function']\n",
        "        #cur_rwd_func = trial.params['reward_function']\n",
        "        cur_lre = decimal_to_text(trial.params['opt_lre'])\n",
        "        cur_buf_size = trial.params['buffer_size']\n",
        "        cur_bat_size = trial.params['batch_size']\n",
        "\n",
        "        ## Create Notation for Hyperparameter Setup\n",
        "        test_name = (f'{cur_n_fcl}FC{cur_fcl_size}_{cur_act_func}_' +\n",
        "                    f'BT{cur_bat_size}BF{cur_buf_size}_Q{cur_q_freq}_' +\n",
        "                    f'TGT{cur_tgt_freq}_LR{cur_lre}')\n",
        "\n",
        "        ## Create Dir to save results\n",
        "        test_name_path =  agent_path + '/' + test_name\n",
        "        if not os.path.exists(test_name_path):\n",
        "            os.makedirs(test_name_path)\n",
        "            print(f\"Directory '{test_name_path}' created successfully.\")\n",
        "        else:\n",
        "            print(f\"Directory '{test_name_path}' already exists.\")\n",
        "\n",
        "        # Create Agent with hyperparameters\n",
        "        best_agent = DdqnAgent(name=agent_name,\n",
        "                            environment=None,\n",
        "                            reward_function = future_profit,\n",
        "                            reward_params = {'n':5},\n",
        "                            env_state_mod_func = env_mod_func,\n",
        "                            input_size= 11,\n",
        "                            hidden_size= sug_hidden_size,\n",
        "                            output_size=3,\n",
        "                            activation_function = sug_activation_function,\n",
        "                            num_hidden_layers = sug_hidden_layers,\n",
        "                            buffer_size= sug_buffer_size,\n",
        "                            batch_size = sug_batch_size,\n",
        "                            alpha = sug_opt_lre,\n",
        "                            gamma = sug_gamma,\n",
        "                            opt_wgt_dcy = 0.01,\n",
        "                            dropout_rate = 0.25,\n",
        "                            device = device)\n",
        "\n",
        "        # Training Model\n",
        "        for key, rl_env in env.items():\n",
        "\n",
        "            if key in trn_keys:\n",
        "                rl_env.add_agent(agent_name)\n",
        "                rl_env.set_decision_agent(agent_name)\n",
        "                if import_tn_csvs:\n",
        "                    timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')    #Requires outside variable\n",
        "                best_agent.set_environment(rl_env)\n",
        "                best_agent.train(start_idx=hyp_trn_idx[key][0],\n",
        "                            end_idx=hyp_trn_idx[key][1],\n",
        "                            training_episodes= 30,\n",
        "                            epsilon_decya_func= linear_decay,\n",
        "                            initial_epsilon= 0.9,\n",
        "                            final_epsilon= 0.1,\n",
        "                            update_q_freq= sug_update_q_freq,\n",
        "                            update_tgt_freq= sug_update_tgt_freq,\n",
        "                            save_path = export_path,\n",
        "                            val_start_idx = hyp_val_idx[key][0],\n",
        "                            val_end_idx = hyp_val_idx[key][1],\n",
        "                            metric_func= metric_function,\n",
        "                            min_training_episodes = 1,\n",
        "                            early_stop = True,\n",
        "                            stop_metric = metric,\n",
        "                            stop_patience = 15,\n",
        "                            stop_delta = 0.001)\n",
        "                rl_env.remove_agent(agent_name)\n",
        "\n",
        "        # Test Model\n",
        "\n",
        "        scores = []\n",
        "        for key, rl_env in env.items():\n",
        "\n",
        "            if key in tst_keys:\n",
        "                rl_env.add_agent(agent_name)\n",
        "                rl_env.set_decision_agent(agent_name)\n",
        "                if import_tn_csvs:\n",
        "                    timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')    #Requires outside variable\n",
        "                best_agent.set_environment(rl_env)\n",
        "                best_agent.test(start_idx = hyp_tst_idx[key][0],\n",
        "                            end_idx = hyp_tst_idx[key][1],\n",
        "                            metric_func= metric_function,\n",
        "                            testing_episodes=1)\n",
        "                rl_env.remove_agent(agent_name)\n",
        "\n",
        "                ## Save Test Metric Result(s) into\n",
        "                ddqn_tst = best_agent.get_testing_episodic_data()\n",
        "                score = ddqn_tst['tot_r'].mean()\n",
        "                scores.append(score)\n",
        "\n",
        "                ## Export Test data\n",
        "                a = rl_env.get_step_data()\n",
        "                b = best_agent.get_step_data()\n",
        "                combined_df = pd.concat([a,b],axis=1)\n",
        "                tst_df_file_name  = f'TST-{key}' + test_name + '.csv'\n",
        "                trn_df_save_path = test_name_path + '/' + tst_df_file_name\n",
        "                combined_df.to_csv(trn_df_save_path)\n",
        "\n",
        "                ## Generate Trading Graphic\n",
        "                tst_graph_file_name = trn_df_save_path[:-4] + '.png'\n",
        "                agentperform.agent_stock_performance(env[key].stock_price_data[hyp_tst_idx[key][0]:hyp_tst_idx[key][1]][:,-1,0], # Selecting all batches, last price of window, closing price\n",
        "                                                    combined_df['Env Action'].to_numpy(),\n",
        "                                                    key,\n",
        "                                                    best_agent.get_name(),\n",
        "                                                    display_graph=False,\n",
        "                                                    save_graphic=True,\n",
        "                                                    path_file=tst_graph_file_name)\n",
        "\n",
        "        mean = np.mean(scores)\n",
        "        return mean\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=100)\n",
        "\n",
        "    print(\"Best value: \", study.best_value)\n",
        "    print(\"Best params: \", study.best_params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwUdEMmhQFnX"
      },
      "source": [
        "# Agent Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95MUF9R5QFnp"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OA5UDlQjQFnp"
      },
      "outputs": [],
      "source": [
        "# Agent Type Setup\n",
        "agent_classes = {'profit_1': DdqnAgent,\n",
        "                 'profit_2': DdqnAgent,\n",
        "                 'random':RandomAgent}\n",
        "\n",
        "# Mul\n",
        "agent_setup = {'profit_1': ['profit_1'],\n",
        "                 'profit_2': ['profit_2'],\n",
        "                 'random': ['random']}\n",
        "                 #final': ['profit', 'risk'], for multi agent key is decision agent\n",
        "                 #'macro': 'macro',\n",
        "                 #'opt': ['profit', 'risk', 'macro']}\n",
        "\n",
        "agent_name_list = list(agent_classes.keys())\n",
        "\n",
        "agents_to_train = ['profit_1', 'profit_2']\n",
        "agents_to_import = {'agent_name': 'path_to_model'}\n",
        "\n",
        "agent_params = {\n",
        "    agent_name_list[0]:{\n",
        "        'name': agent_name_list[0],\n",
        "        'environment': None,\n",
        "        'reward_function': future_profit,\n",
        "        'reward_params': {'n':5},\n",
        "        'env_state_mod_func': None,  #Is Set in Training Loop\n",
        "        'input_size': 15,\n",
        "        'hidden_size': 256,\n",
        "        'output_size':3,\n",
        "        'activation_function': nn.ELU(),\n",
        "        'num_hidden_layers': 2,\n",
        "        'buffer_size': 330,\n",
        "        'batch_size': 75,\n",
        "        'alpha': 0.0005,\n",
        "        'gamma':0.96,\n",
        "        'opt_wgt_dcy': 0,\n",
        "        'dropout_rate': 0.15,\n",
        "        'device': device\n",
        "    },\n",
        "    agent_name_list[1]:{\n",
        "        'name': agent_name_list[1],\n",
        "        'environment': None,\n",
        "        'reward_function': future_profit,\n",
        "        'reward_params': {'n':5},\n",
        "        'env_state_mod_func': None, #Is Set in Training Loop\n",
        "        'input_size': 15,\n",
        "        'hidden_size': 512,\n",
        "        'output_size':3,\n",
        "        'activation_function': nn.ELU(),\n",
        "        'num_hidden_layers': 3,\n",
        "        'buffer_size': 330,\n",
        "        'batch_size': 75,\n",
        "        'alpha': 0.005,\n",
        "        'gamma':0.98,\n",
        "        'opt_wgt_dcy': 0.0,\n",
        "        'dropout_rate': 0.20,\n",
        "        'device': device\n",
        "    },\n",
        "        agent_name_list[2]:{\n",
        "        'name': agent_name_list[2],\n",
        "        'environment': None,\n",
        "        'reward_function': zero_reward,\n",
        "        'reward_params': {},\n",
        "        }}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbtjT_9IQFnq"
      },
      "source": [
        "## Agent Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5Wcjvy7-QFnq"
      },
      "outputs": [],
      "source": [
        "agents_dic = {}\n",
        "\n",
        "for agent_name, agent_class in agent_classes.items():\n",
        "            selected_agent = agent_class(**agent_params[agent_name])\n",
        "            agents_dic[agent_name] = selected_agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4y7pGWBQFnq"
      },
      "source": [
        "# Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nswPoExzQFnr"
      },
      "outputs": [],
      "source": [
        "training_params = {'training_episodes': 3,\n",
        "                   'epsilon_decya_func': linear_decay,\n",
        "                   'initial_epsilon': 0.9,\n",
        "                   'final_epsilon': 0.1,\n",
        "                   'update_q_freq': 1,\n",
        "                   'update_tgt_freq': 25,\n",
        "                   'save_path': export_path,\n",
        "                   'metric_func': metric_function\n",
        "                   }\n",
        "\n",
        "earlystop_params = {'min_training_episodes': 1,\n",
        "                   'early_stop': False,\n",
        "                   'stop_metric': None,\n",
        "                   'stop_patience': 2,\n",
        "                   'stop_delta': 0.001\n",
        "                    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX37f6UKQFnr",
        "outputId": "d5599fb9-5897-4b14-b5fa-b806c405ed42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DJI ENV: Agent profit_1 added\n",
            "DJI ENV: Agent profit_1 assigned as decision agent\n",
            "\n",
            "profit_1: Training Initialized on DJI[0:3244]\n",
            "profit_1: EP 3 of 3 Finished -> ΔQ1 = 107.68, ΔQ2 = 101.39 | ∑R = 2364.09, μR = 0.73 σR = 3.08                                                                                                                                                            \n",
            "profit_1: Training finished on DJI[0:3244]\n",
            "\n",
            "profit_1: Q-Network Exported to file \"C:/programming/MADDQN/test007/profit_1/profit_1\"\n",
            "DJI ENV: Agent profit_1 removed\n",
            "NDAQ ENV: Agent profit_1 added\n",
            "NDAQ ENV: Agent profit_1 assigned as decision agent\n",
            "\n",
            "profit_1: Training Initialized on NDAQ[0:3244]\n",
            "profit_1: EP 3 of 3 Finished -> ΔQ1 = 645.25, ΔQ2 = 752.80 | ∑R = -52.58, μR = -0.02 σR = 5.43                                                                                                                                                            \n",
            "profit_1: Training finished on NDAQ[0:3244]\n",
            "\n",
            "profit_1: Q-Network Exported to file \"C:/programming/MADDQN/test007/profit_1/profit_1\"\n",
            "NDAQ ENV: Agent profit_1 removed\n",
            "SP500 ENV: Agent profit_1 added\n",
            "SP500 ENV: Agent profit_1 assigned as decision agent\n",
            "\n",
            "profit_1: Training Initialized on SP500[0:3244]\n",
            "profit_1: EP 3 of 3 Finished -> ΔQ1 = 2106.09, ΔQ2 = 2804.70 | ∑R = 2967.71, μR = 0.91 σR = 3.23                                                                                                                                                          \n",
            "profit_1: Training finished on SP500[0:3244]\n",
            "\n",
            "profit_1: Q-Network Exported to file \"C:/programming/MADDQN/test007/profit_1/profit_1\"\n",
            "SP500 ENV: Agent profit_1 removed\n",
            "DJI ENV: Agent profit_2 added\n",
            "DJI ENV: Agent profit_2 assigned as decision agent\n",
            "\n",
            "profit_2: Training Initialized on DJI[0:3244]\n",
            "profit_2: EP 3 of 3 Finished -> ΔQ1 = 0.84, ΔQ2 = 0.74 | ∑R = 305.16, μR = 0.09 σR = 2.04                                                                                                                                                                 \n",
            "profit_2: Training finished on DJI[0:3244]\n",
            "\n",
            "profit_2: Q-Network Exported to file \"C:/programming/MADDQN/test007/profit_2/profit_2\"\n",
            "DJI ENV: Agent profit_2 removed\n",
            "NDAQ ENV: Agent profit_2 added\n",
            "NDAQ ENV: Agent profit_2 assigned as decision agent\n",
            "\n",
            "profit_2: Training Initialized on NDAQ[0:3244]\n",
            "profit_2: EP 2 of 3 Finished -> ΔQ1 = 2.04, ΔQ2 = 1.22 | ∑R = 74.73, μR = 0.02 σR = 2.02                                                                                                                                                                  "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train Decision Agent\u001b[39;00m\n\u001b[0;32m     44\u001b[0m agents_dic[decision_agent]\u001b[38;5;241m.\u001b[39mset_env_stat_modify_func(env_mod_func_dic[key]\u001b[38;5;241m.\u001b[39mprocess)\n\u001b[1;32m---> 45\u001b[0m agents_dic[decision_agent]\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcombined_params)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Save Agent\u001b[39;00m\n\u001b[0;32m     48\u001b[0m save_agent_path \u001b[38;5;241m=\u001b[39m save_path_root \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecision_agent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "File \u001b[1;32mc:\\Programming\\MADDQN\\agents\\ddqn.py:271\u001b[0m, in \u001b[0;36mDdqnAgent.train\u001b[1;34m(self, start_idx, end_idx, training_episodes, epsilon_decya_func, initial_epsilon, final_epsilon, val_start_idx, val_end_idx, save_path, early_stop, min_training_episodes, metric_func, metric_func_arg, stop_metric, stop_patience, stop_delta, update_q_freq, update_tgt_freq)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, training_episodes\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    266\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m epsilon_decya_func(initial_epsilon, \n\u001b[0;32m    267\u001b[0m                                     final_epsilon,\n\u001b[0;32m    268\u001b[0m                                     episode_num,\n\u001b[0;32m    269\u001b[0m                                     training_episodes)\n\u001b[1;32m--> 271\u001b[0m     tot_reward, mean_reward, std_reward, loss, trn_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_play_episode(epsilon, update_q_freq, update_tgt_freq, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Rewards based on Validation Set\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate:\n",
            "File \u001b[1;32mc:\\Programming\\MADDQN\\agents\\ddqn.py:394\u001b[0m, in \u001b[0;36mDdqnAgent._play_episode\u001b[1;34m(self, epsilon, update_q_freq, update_tgt_freq, step_type)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tgt_nn_update_bool:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ1_tgt_nn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_tgt_nn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ1_nn,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 394\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ2_tgt_nn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_tgt_nn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ2_nn,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid step_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Programming\\MADDQN\\agents\\ddqn.py:457\u001b[0m, in \u001b[0;36mDdqnAgent._create_tgt_nn\u001b[1;34m(self, Q_nn, device)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03mCreate a deep copy of the Q_nn for the target network.\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;66;03m# Serialize the original model's state\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m target_network \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(Q_nn)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# Move the copied model to the specified device\u001b[39;00m\n\u001b[0;32m    460\u001b[0m target_network\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m _reconstruct(x, memo, \u001b[38;5;241m*\u001b[39mrv)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m deepcopy(state, memo)\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(x, memo)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m deepcopy(value, memo)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m _reconstruct(x, memo, \u001b[38;5;241m*\u001b[39mrv)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m deepcopy(state, memo)\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(x, memo)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m deepcopy(value, memo)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m _reconstruct(x, memo, \u001b[38;5;241m*\u001b[39mrv)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[0;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[1;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m deepcopy(value, memo)\n\u001b[0;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(x, memo)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m deepcopy(value, memo)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(memo)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\beckm\\anaconda3\\envs\\MADDQN\\Lib\\site-packages\\torch\\_tensor.py:172\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default implementation of __deepcopy__() for quantized tensors \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpects the tensor returned by torch._utils._rebuild_qtensor() to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch the type of the instance being copied. If you encounter this, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease open an issue on PyTorch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms GitHub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m         )\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     new_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_empty([])\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensor) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default implementation of __deepcopy__() for non-wrapper subclasses \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly works for subclass types that implement new_empty() and for which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man instance of a different type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m         )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if train_agent:\n",
        "    \n",
        "    # Export Training Parameters\n",
        "    train_param_file_name = 'train_params.txt'\n",
        "\n",
        "    with open(train_param_file_name, 'w') as file:\n",
        "        for key, value in training_params.items():\n",
        "            file.write(f'Agents:\\t {agent_name_list}\\n')\n",
        "            file.write(f'Agents Parameters:\\n\\n {agent_params}\\n')\n",
        "            file.write(f'Training Keys:\\t {trn_keys}\\n')\n",
        "            file.write(f'Training Range:\\t {training_range}\\n')\n",
        "            file.write(f'Trainig Parameters:\\n\\n {training_params}\\n')\n",
        "            file.write(f'Early Stop Parameters:\\n\\n {earlystop_params}\\n')\n",
        "            if validate:\n",
        "                file.write(f'Validation Keys:\\t {val_keys}\\n')\n",
        "                file.write(f'Validation Range:\\t {validation_range}\\n')\n",
        "            file.write(f'Env State Modification parmas:\\n {env_mod_parms}\\n')\n",
        "            file.write(f'scaling_range:\\t {training_range}\\n')\n",
        "\n",
        "    # Which Agents to Train\n",
        "    filtered_agents = {\n",
        "        decision_agent: agents_in_setup\n",
        "        for decision_agent, agents_in_setup in agent_setup.items()\n",
        "        if decision_agent in agents_to_train\n",
        "    }\n",
        "\n",
        "    for decision_agent, agents_in_setup in filtered_agents.items():\n",
        "        for key in trn_keys:\n",
        "            rl_env = env[key]\n",
        "\n",
        "            if validate:\n",
        "                idx_params = {'start_idx': trn_idx[key][0],\n",
        "                              'end_idx':trn_idx[key][1],\n",
        "                              'val_start_idx': val_idx[key][0],\n",
        "                              'val_end_idx': val_idx[key][1]}\n",
        "            else:\n",
        "                idx_params ={'start_idx': trn_idx[key][0],\n",
        "                             'end_idx':trn_idx[key][1],\n",
        "                             'val_start_idx': None,\n",
        "                             'val_end_idx': None}\n",
        "\n",
        "            combined_params = training_params | earlystop_params | idx_params\n",
        "\n",
        "            # Setup agents with environment\n",
        "            for agent in agents_in_setup:\n",
        "                rl_env.add_agent(agent)\n",
        "                agents_dic[agent].set_environment(rl_env)\n",
        "            rl_env.set_decision_agent(decision_agent)\n",
        "\n",
        "            # Train Sub-subagents\n",
        "            for agent in agents_in_setup:\n",
        "                if agent is not decision_agent:\n",
        "                    agents_dic[agent].set_env_stat_modify_func(env_mod_func_dic[key].process)\n",
        "                    agents_dic[agent].train(**combined_params)\n",
        "                    # Save Agent\n",
        "                    save_agent_path = save_path_root + f'/{agent}/'\n",
        "                    os.makedirs(save_agent_path, exist_ok=True)\n",
        "                    agents_dic[agent].export_Q_nn(save_agent_path)\n",
        "\n",
        "            # Train Decision Agent\n",
        "            agents_dic[decision_agent].set_env_stat_modify_func(env_mod_func_dic[key].process)\n",
        "            agents_dic[decision_agent].train(**combined_params)\n",
        "\n",
        "            # Save Agent\n",
        "            save_agent_path = save_path_root + f'/{decision_agent}/'\n",
        "            os.makedirs(save_agent_path, exist_ok=True)\n",
        "            file_root_name = f'{key}_TRN_{trn_idx[key][0]}-{trn_idx[key][1]}'\n",
        "            agents_dic[decision_agent].export_Q_nn(save_agent_path + decision_agent)\n",
        "\n",
        "            # Create/Export Records\n",
        "            env_data_record = rl_env.get_step_data()\n",
        "            agent_data_record = agents_dic[decision_agent].get_step_data()\n",
        "            training_record =  pd.concat([env_data_record, agent_data_record], axis=1, join='inner')\n",
        "            training_record.to_csv(f'{save_agent_path}{file_root_name}_step_data.csv')\n",
        "            episodic_training_record = agents_dic[decision_agent].get_training_episodic_data()\n",
        "            episodic_training_record.to_csv(f'{save_agent_path}{file_root_name}_epi_data.csv')\n",
        "\n",
        "            # Remove Agent\n",
        "            for agent in agents_in_setup:\n",
        "                rl_env.remove_agent(agent)\n",
        "                agents_dic[agent].set_environment(None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVcrJ5qVQFns"
      },
      "source": [
        "# Agent Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc6ZYhOsQFns"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qegyTiiIQFns"
      },
      "outputs": [],
      "source": [
        "testing_params = {DdqnAgent: {\n",
        "                   'metric_func': metric_function,\n",
        "                   'metric_func_arg': {},\n",
        "                   'testing_episodes':1},\n",
        "                  RandomAgent: {\n",
        "                    'metric_func': metric_function,\n",
        "                   'metric_func_arg': {},\n",
        "                   'testing_episodes':100}}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rF1E25JQFns"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8buSLH7yQFnt"
      },
      "outputs": [],
      "source": [
        "if test_agent:\n",
        "    result_dic_struct = ['stock','agent','test_interval','test_num']\n",
        "    results = {}\n",
        "\n",
        "    for key in tst_keys:\n",
        "\n",
        "        # Init Record[Stock]\n",
        "        results[key] = {}\n",
        "        test_key = f'{tst_idx[key][0]}:{tst_idx[key][1]}'\n",
        "        stock_price_data = env_data[key]['rw_raw_price_env'][tst_idx[key][0]:tst_idx[key][1],-1,0]\n",
        "        rl_env = env[key]\n",
        "\n",
        "        for decision_agent, agents_in_setup in agent_setup.items():\n",
        "\n",
        "            # Init Record[Stock][Agent]\n",
        "            results[key][decision_agent] = {}\n",
        "\n",
        "            # Init Record[Stock][Agent][test_interval]\n",
        "            results[key][decision_agent][test_key] = {}   # Different Test Keys will need loop\n",
        "\n",
        "            # Setup agents with environment\n",
        "            for agent in set([decision_agent] + agents_in_setup):\n",
        "                rl_env.add_agent(agent)\n",
        "                agents_dic[agent].set_environment(rl_env)\n",
        "            rl_env.set_decision_agent(decision_agent)\n",
        "\n",
        "            # Enable Randomess if Agent is of class RandomAgent\n",
        "            if isinstance(agents_dic[decision_agent], RandomAgent):\n",
        "                new_random_seed = random.randint(1, 10**9)\n",
        "                set_seed(new_random_seed)\n",
        "\n",
        "            # Save Agent\n",
        "            save_agent_path = save_path_root + f'/{decision_agent}/'\n",
        "            os.makedirs(save_agent_path, exist_ok=True)\n",
        "            file_root_name = f'{key}_TST_{tst_idx[key][0]}-{tst_idx[key][1]}'\n",
        "\n",
        "            # Test Decision Agent\n",
        "            params = testing_params[agent_classes[decision_agent]]\n",
        "            if not isinstance(agents_dic[decision_agent], RandomAgent):\n",
        "                agents_dic[agent].set_env_stat_modify_func(env_mod_func_dic[key].process)\n",
        "            agents_dic[decision_agent].test(start_idx=tst_idx[key][0],\n",
        "                                            end_idx=tst_idx[key][1],\n",
        "                                            **params)\n",
        "\n",
        "            # Generate Testing Records\n",
        "            env_data_record = rl_env.get_step_data()\n",
        "            agent_data_record = agents_dic[decision_agent].get_step_data()\n",
        "            test_record =  pd.concat([env_data_record, agent_data_record], axis=1, join='inner')\n",
        "            test_record.to_csv(f'{save_agent_path}{file_root_name}_step_data.csv')\n",
        "            episodic_testing_record = agents_dic[decision_agent].get_testing_episodic_data()\n",
        "            episodic_testing_record.to_csv(f'{save_agent_path}{file_root_name}_epi_data.csv')\n",
        "\n",
        "            trade_actions_per_test = episodic_testing_record['tst_actions']\n",
        "\n",
        "            for idx, action_set in enumerate(trade_actions_per_test):\n",
        "                file_root_name = f'{key}_TST_{tst_idx[key][0]}-{tst_idx[key][1]}_{[idx]}'\n",
        "                test_metrics = agentperform.agent_stock_performance(stock_price_ts=np.array(stock_price_data),\n",
        "                                                                    trade_ts=np.array(action_set),\n",
        "                                                                    stock_name=key,\n",
        "                                                                    agent_name=decision_agent,\n",
        "                                                                    display_graph=False,\n",
        "                                                                    save_graphic= True,\n",
        "                                                                    path_file = f'{save_agent_path}{file_root_name}.png')\n",
        "                del test_metrics['stock']\n",
        "                del test_metrics['agent_name']\n",
        "                results[key][decision_agent][test_key][idx] = test_metrics\n",
        "\n",
        "            # Remove Agent\n",
        "            for agent in set([decision_agent] + agents_in_setup):\n",
        "                rl_env.remove_agent(agent)\n",
        "                agents_dic[agent].set_environment(None)\n",
        "\n",
        "    display(results)\n",
        "    set_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFAgjjt1QFnt"
      },
      "source": [
        "# Aggreating Test Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BApRg8hXQFnt"
      },
      "outputs": [],
      "source": [
        "if test_agent:\n",
        "    aggerate_results = {}\n",
        "    for agent in agent_name_list:\n",
        "        aggerate_results[agent] = {}\n",
        "        for stock in tst_keys:\n",
        "            aggerate_results[agent][stock] = {}\n",
        "            test_key = f'{tst_idx[stock][0]}:{tst_idx[stock][1]}'\n",
        "            aggerate_results[agent][stock][test_key] = {}\n",
        "            values = np.empty((0,len(metrics)))\n",
        "            for test_num in range(testing_params[agent_classes[agent]]['testing_episodes']):\n",
        "\n",
        "                values_array = [results[stock][agent][test_key][test_num][key] for key in metrics]\n",
        "                current_values = np.array(values_array)\n",
        "                values = np.vstack((values,current_values))\n",
        "\n",
        "                means_for_metrics = np.mean(values, axis=0)\n",
        "                std_for_metrics = np.std(values, axis=0)\n",
        "\n",
        "            for idx,metric in enumerate(metrics):\n",
        "                aggerate_results[agent][stock][test_key][metric] = (means_for_metrics[idx],std_for_metrics[idx])\n",
        "\n",
        "    summarized_aggerate_results = {}\n",
        "\n",
        "    for metric in metrics:\n",
        "        model_list = []\n",
        "        dataset_name = []\n",
        "        scores = []\n",
        "        for agent in aggerate_results.keys():\n",
        "\n",
        "            model_list.append(agent)\n",
        "            score_list = []\n",
        "            for stock in aggerate_results[agent].keys():\n",
        "                for test in aggerate_results[agent][stock].keys():\n",
        "                    run_name = stock + \"-\" + test\n",
        "                    if run_name not in dataset_name:\n",
        "                        dataset_name.append(run_name)\n",
        "                    score = aggerate_results[agent][stock][test][metric][0]\n",
        "                    score_list.append(np.round(score,2))\n",
        "            scores.append(score_list)\n",
        "\n",
        "        score_array = np.array(scores).T\n",
        "\n",
        "        df = pd.DataFrame(score_array,columns=model_list)\n",
        "        df['dataset'] = dataset_name\n",
        "\n",
        "        column_order = ['dataset'] + [col for col in df.columns if col != 'dataset']\n",
        "        df = df[column_order]\n",
        "        summarized_aggerate_results[metric] = df\n",
        "\n",
        "\n",
        "        # Export Aggreate Date to CSV\n",
        "        means = df[model_list].mean()\n",
        "        model_means = {model: means[model] for model in model_list}\n",
        "        model_means.update({'dataset': 'mean'})\n",
        "        df_export = df.copy()\n",
        "        df_export.loc[len(df)] = model_means\n",
        "        df_export.to_csv(f'{save_path_root}/{metric}_agg_data.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    display(summarized_aggerate_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0ZPtDYFQFnu"
      },
      "source": [
        "# Significance Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDyd0N45QFnu"
      },
      "outputs": [],
      "source": [
        "if test_agent:\n",
        "    for metric in metrics:\n",
        "        display(metric)\n",
        "        display(summarized_aggerate_results[metric])\n",
        "        test = prob_evaluate.generate_rank_array_from_dataframe(summarized_aggerate_results[metric],\n",
        "                                                                model_list,equal_rank_behav=\"mean\",\n",
        "                                                                rank_order=aval_metrics_rank_dic[metric])\n",
        "        display(test)\n",
        "        stat, critical_f_value, reject_null_hypo, k, n, pvalue = prob_evaluate.iman_davenport_test(test,0.95,arr_order='rows')\n",
        "        display(f'n models: {k}, n_datasets {n}, ImanDavenport Stat: {stat}, Critical Value: {critical_f_value}, pvalue: {pvalue}, Reject Null Hypoth: {reject_null_hypo}')\n",
        "\n",
        "        # Create a dictionary with the output values\n",
        "        result_dict = {\n",
        "                    'n_models': [k],\n",
        "                    'n_datasets': [n],\n",
        "                    'Iman_Davenport_Stat': [stat],\n",
        "                    'Critical_Value': [critical_f_value],\n",
        "                    'pvalue': [pvalue],\n",
        "                    'Reject_Null_Hypothesis': [reject_null_hypo]}\n",
        "\n",
        "        result_df = pd.DataFrame(result_dict)\n",
        "        result_df.to_csv(f'{save_path_root}/{metric}_iman_davenport_test.csv')\n",
        "\n",
        "        # Nemenyi Test\n",
        "        results_raw = prob_evaluate.nemenyi_test(test,0.95,model_list)\n",
        "        # Formating Output\n",
        "        df_export = pd.DataFrame(results_raw, columns=['agent1_agent2', 'nemenyi_stat', 'nemenyi_threshold', 'reject_null_hypo'])\n",
        "        df_export[['agent1', 'agent2']] = pd.DataFrame(df_export['agent1_agent2'].tolist(), index=df_export.index)\n",
        "        df_export = df_export.drop(columns=['agent1_agent2'])\n",
        "        new_column_order = ['agent1', 'agent2', 'nemenyi_stat', 'nemenyi_threshold', 'reject_null_hypo']\n",
        "        df_export = df_export[new_column_order]\n",
        "        df_export.to_csv(f'{save_path_root}/{metric}_nemenyi_test.csv')\n",
        "\n",
        "        display(df_export)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAx4C23NWsie"
      },
      "source": [
        "#Move records to Google Drive for VM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AVXuuyVUJtT"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "  import shutil\n",
        "  from google.colab import drive\n",
        "\n",
        "  # Connect Google Drive to download records for autoclosing of VM\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  # Define your directory path and case name\n",
        "  directory_path = save_path_root\n",
        "  zip_file_name = case_name.replace(\"/\", \"\")\n",
        "  output_filename = f'{zip_file_name}.zip'\n",
        "\n",
        "  # Create a zip file of the directory\n",
        "  shutil.make_archive(zip_file_name, 'zip', directory_path)\n",
        "\n",
        "  # Move the zip file to Google Drive\n",
        "  shutil.move(f'{zip_file_name}.zip', f'/content/drive/My Drive/{output_filename}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2THhlh2QFnv"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "90L1szCuQFnU"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
