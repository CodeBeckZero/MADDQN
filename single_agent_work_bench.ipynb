{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in another environment (e.g., VS Code)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Code specific to Google Colab\n",
    "    print(\"Running in Google Colab\")\n",
    "\n",
    "    # Additional setup commands for Colab\n",
    "    !pip install neuralforecast\n",
    "    !pip install gymnasium\n",
    "    !pip install QuantStats\n",
    "else:\n",
    "    # Code for other environments (e.g., VS Code)\n",
    "    print(\"Running in another environment (e.g., VS Code)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install RL Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Retrive required files\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/environments/stockenv.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/cleandata.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/data.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/epsilon_decay.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/agentperform.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/prob_evaluate.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/ddqn.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/random.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/baseagent.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/rewards/stockmarket.py            \n",
    "    # Move all directories and files from content/raw.githubusercontent.com to content/\n",
    "    !mv /content/raw.githubusercontent.com/* /content/\n",
    "\n",
    "    # Delete the raw.githubusercontent.com directory\n",
    "    !rm -rf /content/raw.githubusercontent.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activate Python Libraries & Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is not available. PyTorch is running on CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import optuna\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import utilities.agentperform as agentperform\n",
    "import utilities.cleandata as cln \n",
    "from utilities.epsilon_decay import linear_decay\n",
    "from utilities.data import UniStockEnvDataStruct, TimesNetProcessing\n",
    "from utilities import prob_evaluate\n",
    "from agents.ddqn import DdqnAgent\n",
    "from agents.random import RandomAgent\n",
    "from rewards.stockmarket import future_profit, risk_reward, zero_reward\n",
    "from environments.stockenv import ContinuousOHLCVEnv\n",
    "from datetime import datetime\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import TimesNet\n",
    "from neuralforecast.losses.numpy import mae, mse\n",
    "import logging\n",
    "\n",
    "\n",
    "# \n",
    "logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").addHandler(logging.NullHandler())\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").addHandler(logging.NullHandler())\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1' # Prevent Warning \n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    # Python random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # If you are using CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "        # Additional settings to force determinism in your operations:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"CUDA (GPU support) is not available. PyTorch is running on CPU.\")\n",
    "\n",
    "\n",
    "def decimal_to_text(decimal_number):\n",
    "    # Remove the decimal point and convert to integer\n",
    "    integer_part = int(decimal_number * 1000)\n",
    "    # Convert the integer to text\n",
    "    text_representation = str(integer_part)\n",
    "    return text_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters & CSV Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "\n",
    "if not IN_COLAB:\n",
    "    pwd = \"C:/programming/MADDQN\"\n",
    "    sys.path.append(pwd)\n",
    "    \n",
    "    # Output Path Location for CSV export\n",
    "    export_path = pwd + \"/output_data/test001/\"\n",
    "\n",
    "# Input Data Location, File Name, Stock name for labels\n",
    "input_url = 'https://raw.githubusercontent.com/CodeBeckZero/MADDQN/main/input_data'\n",
    "\n",
    "stock_inputs ={'DJI':'^DJI_daily.csv',\n",
    "               'NDAQ': '^IXIC_daily.csv',\n",
    "               'SP500': '^SPX_daily.csv',\n",
    "               'AAPL': 'AAPL_daily.csv',\n",
    "               'AMZN': 'AMZN_daily.csv',\n",
    "               'GOOGL': 'GOOGL_daily.csv',\n",
    "               'MSFT': 'MSFT_daily.csv',\n",
    "               'SINE': 'sine_wave_daily.csv',\n",
    "               'FORD': 'F_daily.csv',\n",
    "               'JNJ': 'JNJ_daily.csv',\n",
    "               'NEE': 'NEE_daily.csv',\n",
    "               'PFE': 'PFE_daily.csv',\n",
    "               'TSLA': 'TSLA_daily.csv',\n",
    "               'COKE': 'COKE_daily.csv',\n",
    "               'PG': 'PG_daily.csv'}\n",
    "\n",
    "# Training Inputs\n",
    "trn_keys = ['DJI','NDAQ','SP500']\n",
    "\n",
    "# Validation Inputs\n",
    "val_keys = trn_keys\n",
    "\n",
    "# Testing Inputs\n",
    "tst_keys = ['AAPL','AMZN','GOOGL','MSFT','FORD','JNJ','NEE','PFE','TSLA','COKE','PG']\n",
    "\n",
    "window_size = 28 # Needs to match the size Timesnet is trained on\n",
    "price_based_on = 'close'\n",
    "columns = ['open','high','low','close','volume']\n",
    "\n",
    "\n",
    "# Metrics Interested in\n",
    "metrics = ['n_trades','n_wins', 'win_percentage','cumulative_return','sortino','max_drawdown','sharpe', 'trade_dur_avg']\n",
    "\n",
    "aval_metrics_rank_dic = {'n_trades':'max','n_wins': 'max' ,'n_losses':'max','win_percentage':'max','cumulative_return':'max', \n",
    "                 'sortino':'max','max_drawdown':'min', 'sharpe':'max', 'trade_dur_avg':'max', 'trade_dur_min':'max',\n",
    "                 'trade_dur_max':'max','buy_hold':'max'}\n",
    "## See agentperform.py -> results dictionary for options\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Enviornment Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_data = {}\n",
    "env = {}\n",
    "\n",
    "for stock, file in stock_inputs.items():\n",
    "    if stock in set(trn_keys + val_keys + tst_keys):\n",
    "        # Import\n",
    "        df = cln.YAHOO_csv_input(file, input_url)\n",
    "        data_dic = UniStockEnvDataStruct(df,price_based_on,window_size)\n",
    "        env_data[stock] = data_dic\n",
    "        env[stock] = ContinuousOHLCVEnv(name=stock,\n",
    "                                        ohlcv_data = env_data[stock]['rw_raw_env'] ,\n",
    "                                        stock_price_data= env_data[stock]['rw_raw_price_env'],\n",
    "                                        commission_rate=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbench Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_name = 't'\n",
    "\n",
    "# Timesnet\n",
    "## Number of price predictions in the Future by TimesNet (Required for RL agent's input layer)\n",
    "n_prediction = 5\n",
    "## Need to Train TimesNet Preprocessing model (processed every cycle)                            \n",
    "train_tn_model = False \n",
    "## Importing TimesNet Preprocessing model (processed every cycle)            \n",
    "import_tn_model = False                     \n",
    "tn_model_path = pwd + '/gen_data/timesnet/'\n",
    "## Use Imported CSVs from Preprocessing model (no processing, straight to RL agent)\n",
    "import_tn_csvs = True                       \n",
    "tn_csvs_path = pwd + '/gen_data/csvs/'   \n",
    "## No modifaction of environmental state before input to agent\n",
    "no_tn_preprocessing = False                \n",
    "\n",
    "# Limited Exploratory Hyperparmater Discover for single RL Agent\n",
    "hyperparam_discovery = True                \n",
    "\n",
    "# Traditional Training/Testing\n",
    "\n",
    "case_name = '/test001'\n",
    "save_path_root = pwd + case_name\n",
    "## Train Agent(s)\n",
    "train_agent = True                          \n",
    "# Test Agent(s)\n",
    "test_agent = True                           \n",
    "\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesnet = TimesNetProcessing(env_data)\n",
    "\n",
    "env_mod_func_dic = {'train': timesnet.process,\n",
    "               'import':timesnet.process,\n",
    "               'csv':timesnet.csv_process,\n",
    "               'none': None}\n",
    "\n",
    "if  train_tn_model ^ import_tn_model ^ import_tn_csvs ^ no_tn_preprocessing:\n",
    "    if train_tn_model or import_tn_model:\n",
    "        env_mod_func = env_mod_func_dic['train']\n",
    "    if import_tn_csvs:\n",
    "        env_mod_func = env_mod_func_dic['csv']\n",
    "    if no_tn_preprocessing:\n",
    "        env_mod_func = env_mod_func_dic['none']\n",
    "else:\n",
    "    raise ValueError(\"Only one TimesNet Preprocessing Options can be selected\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Function\n",
    "\n",
    "Function that generates metric from enviornment that will be used during validation phase of training or testing phase of model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_function(env):\n",
    "    metric = env.step_info[-1]['New Portfolio Value'] -  env.step_info[-1]['Portfolio Value']\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimesNet Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_tn_model:\n",
    "    \n",
    "    model = TimesNet(h = n_prediction,          # Forecast horizon\n",
    "                    input_size = window_size,   # Length of Batches\n",
    "                    batch_size = 1,             # Number of timeseries to predict\n",
    "                    #futr_exog_list = remaining_columns,\n",
    "                    hidden_size = 128,          # Size of embedding for embedding and encoders,\n",
    "                    dropout = 0.40,             # Dropout for embeddings\n",
    "                    conv_hidden_size = 3,       # Channels for the inception block\n",
    "                    top_k = 5,                  # Top num of periods from FFT considered\n",
    "                    num_kernels = 13,           # number of kernels for the inception block\n",
    "                    encoder_layers = 3,         # num of encoders\n",
    "                    max_steps = 1000,           # of training steps\n",
    "                    early_stop_patience_steps = 10, #early stoppage on validation\n",
    "                    val_check_steps = 100,      # Val check every X steps,\n",
    "                    windows_batch_size = 150,   # Number of windows in training epoch,\n",
    "                    num_workers_loader = 7,\n",
    "                    learning_rate = 0.0003,\n",
    "                    random_seed = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_tn_model:\n",
    "  nf = NeuralForecast(models=[model], freq='d')\n",
    "  results = {}\n",
    "  for key in trn_keys:\n",
    "    results[key] = nf.fit(df=env[key],val_size=0.2)\n",
    "\n",
    "  nf.save(path= tn_model_path,\n",
    "          model_index=None,\n",
    "          overwrite=True,\n",
    "          save_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_tn_model:\n",
    "# Define the correct path\n",
    "  if IN_COLAB:\n",
    "    \n",
    "    model_path = os.path.join(os.getcwd(), 'gen_data', 'timesnet')\n",
    "\n",
    "    # Ensure the directory and file exist\n",
    "    if os.path.exists(model_path):\n",
    "        nf = NeuralForecast.load(path=model_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model path {model_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_tn_csvs:\n",
    "\n",
    "    if IN_COLAB:\n",
    "        # Input Data Location, File Name, Stock name for labels\n",
    "        csv_path = 'https://raw.githubusercontent.com/CodeBeckZero/MADDQN/main/gen_data/csvs/'\n",
    "\n",
    "    else:\n",
    "        csv_path  = tn_csvs_path\n",
    "\n",
    "    stock_tn ={'DJI':'DJI_tn.csv',\n",
    "                'NDAQ': 'NDAQ_tn.csv',\n",
    "                'SP500': 'SP500_tn.csv',\n",
    "                'AAPL': 'AAPL_tn.csv',\n",
    "                'AMZN': 'AMZN_tn.csv',\n",
    "                'GOOGL': 'GOOGL_tn.csv',\n",
    "                'MSFT': 'MSFT_tn.csv',\n",
    "                'FORD': 'FORD_tn.csv',\n",
    "                'JNJ': 'JNJ_tn.csv',\n",
    "                'NEE': 'NEE_tn.csv',\n",
    "                'PFE': 'PFE_tn.csv',\n",
    "                'TSLA': 'TSLA_tn.csv',\n",
    "                'COKE': 'COKE_tn.csv',\n",
    "                'PG': 'PG_tn.csv'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Hyperparameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparam_discovery:\n",
    "    # Training Inputs\n",
    "    hyp_training_range = ('2007-01-01','2020-12-31')\n",
    "    hyp_trn_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_training_range]\n",
    "\n",
    "    # Validation Inputs\n",
    "    hyp_validation_range = ('2021-01-01', '2021-12-31')\n",
    "    hyp_val_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_validation_range]\n",
    "\n",
    "    # Testing Inputs\n",
    "    hyp_testing_range = ('2021-01-01', '2023-12-31')\n",
    "    hyp_tst_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_testing_range]\n",
    "\n",
    "    hyp_trn_idx = {}\n",
    "    hyp_val_idx = {}\n",
    "    hyp_tst_idx = {}\n",
    "\n",
    "    for stock, file in stock_inputs.items():\n",
    "        if stock in set(trn_keys + val_keys + tst_keys):\n",
    "            if stock in trn_keys:\n",
    "                hyp_trn_idx[stock] = env_data[stock].gen_rw_idxs(hyp_trn_dt_range)\n",
    "            if stock in val_keys:\n",
    "                hyp_val_idx[stock] = env_data[stock].gen_rw_idxs(hyp_val_dt_range)\n",
    "            if stock in tst_keys:\n",
    "                hyp_tst_idx[stock] = env_data[stock].gen_rw_idxs(hyp_tst_dt_range)\n",
    "\n",
    "    display(hyp_trn_idx,hyp_val_idx,hyp_tst_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Search & Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparam_discovery:\n",
    "    \n",
    "    # For Objective function, need to create agent name before to link agent with enviornment\n",
    "    agent_name = 'hyp_discovery_agent'\n",
    "    agent_path = export_path + '/' + agent_name\n",
    "    metric = 'val_tot_r'\n",
    "    max_len_buf = np.round(hyp_trn_idx['DJI'][1] - hyp_trn_idx['DJI'][0] + window_size, -2) -10 # manual input, could be error here if \n",
    "    print(f'Max Mem Length: {max_len_buf}')\n",
    "           \n",
    "    def objective(trial):\n",
    "    \n",
    "        # Define the hyperparameters to search over\n",
    "        \n",
    "        ## NN hyperparameters\n",
    "        sug_hidden_layers = trial.suggest_int('hidden_layers', low=1, high=3)\n",
    "        sug_hidden_size = trial.suggest_int('hidden_size', low=64, high=512, step=64)\n",
    "        sug_update_q_freq = trial.suggest_int('update_q_freq',low=1,high=5)\n",
    "        sug_update_tgt_freq = trial.suggest_int('update_tgt_freq',low=5,high=15)\n",
    "        \n",
    "        ## Activation Function Passing\n",
    "        activation_functions = {\n",
    "        'LRELUd': nn.LeakyReLU(),\n",
    "        'LRELUs02': nn.LeakyReLU(negative_slope=0.2),\n",
    "        'GELU': nn.GELU(),\n",
    "        'TANH': nn.Tanh(),\n",
    "        'SELU':nn.SELU(),\n",
    "        'SILU': nn.SiLU()\n",
    "        }\n",
    "        sug_activation_function_name = trial.suggest_categorical('activation_function', list(activation_functions.keys()))\n",
    "        sug_activation_function = activation_functions[sug_activation_function_name]\n",
    "        \n",
    "        \"\"\"\n",
    "        ## Reward Function Passing\n",
    "        reward_functions = {\n",
    "        'profit': future_profit(None,5),\n",
    "        'risk': risk_reward(None,5),\n",
    "        }\n",
    "        sug_reward_function_name = trial.suggest_categorical('reward_function', list(reward_functions.keys()))\n",
    "        sug_reward_function = reward_functions[sug_reward_function_name]\n",
    "        \"\"\"\n",
    "        ## Optimizer hyperparameters\n",
    "        sug_opt_lre = trial.suggest_categorical('opt_lre',[0.0001,0.0005,0.001, 0.005, 0.01, 0.05, 0.1])\n",
    "        sug_gamma = trial.suggest_float('gamma',low=0.90,high=0.99,step=0.01)\n",
    "        ## Memory Replay hyperparameters\n",
    "        sug_buffer_size = trial.suggest_int('buffer_size',low=100,high=max_len_buf,step=10)\n",
    "        sug_batch_size = trial.suggest_int('batch_size',low=10,high=sug_buffer_size,step=5)\n",
    "        \n",
    "        # Saving Setup\n",
    "        ## Current Parameter Values:\n",
    "        cur_n_fcl = trial.params['hidden_layers']\n",
    "        cur_fcl_size = trial.params['hidden_size']\n",
    "        cur_q_freq = trial.params['update_q_freq']\n",
    "        cur_tgt_freq = trial.params['update_tgt_freq']\n",
    "        cur_act_func = trial.params['activation_function']\n",
    "        #cur_rwd_func = trial.params['reward_function']\n",
    "        cur_lre = decimal_to_text(trial.params['opt_lre'])\n",
    "        cur_buf_size = trial.params['buffer_size']\n",
    "        cur_bat_size = trial.params['batch_size']\n",
    "        \n",
    "        ## Create Notation for Hyperparameter Setup    \n",
    "        test_name = (f'{cur_n_fcl}FC{cur_fcl_size}_{cur_act_func}_' +\n",
    "                    f'BT{cur_bat_size}BF{cur_buf_size}_Q{cur_q_freq}_' +\n",
    "                    f'TGT{cur_tgt_freq}_LR{cur_lre}')\n",
    "        \n",
    "        ## Create Dir to save results\n",
    "        test_name_path =  agent_path + '/' + test_name \n",
    "        if not os.path.exists(test_name_path):\n",
    "            os.makedirs(test_name_path)\n",
    "            print(f\"Directory '{test_name_path}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Directory '{test_name_path}' already exists.\")\n",
    "        \n",
    "        # Create Agent with hyperparameters  \n",
    "        best_agent = DdqnAgent(name=agent_name,\n",
    "                            environment=None,\n",
    "                            reward_function = future_profit,\n",
    "                            reward_params = {'n':5},\n",
    "                            env_state_mod_func = env_mod_func,     \n",
    "                            input_size= 11,\n",
    "                            hidden_size= sug_hidden_size, \n",
    "                            output_size=3, \n",
    "                            activation_function = sug_activation_function,\n",
    "                            num_hidden_layers = sug_hidden_layers,                  \n",
    "                            buffer_size= sug_buffer_size, \n",
    "                            batch_size = sug_batch_size,\n",
    "                            alpha = sug_opt_lre,\n",
    "                            gamma = sug_gamma,\n",
    "                            opt_wgt_dcy = 0.01,\n",
    "                            dropout_rate = 0.25,                \n",
    "                            device = device)\n",
    "        \n",
    "        # Training Model\n",
    "        for key, rl_env in env.items():\n",
    "            \n",
    "            if key in trn_keys:\n",
    "                rl_env.add_agent(agent_name)\n",
    "                rl_env.set_decision_agent(agent_name)\n",
    "                if import_tn_csvs:\n",
    "                    timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')    #Requires outside variable         \n",
    "                best_agent.set_environment(rl_env)\n",
    "                best_agent.train(start_idx=hyp_trn_idx[key][0],\n",
    "                            end_idx=hyp_trn_idx[key][1],\n",
    "                            training_episodes= 1,\n",
    "                            epsilon_decya_func= linear_decay,\n",
    "                            initial_epsilon= 0.9,\n",
    "                            final_epsilon= 0.1,\n",
    "                            update_q_freq= sug_update_q_freq,\n",
    "                            update_tgt_freq= sug_update_tgt_freq,\n",
    "                            save_path = export_path,\n",
    "                            val_start_idx = hyp_val_idx[key][0],\n",
    "                            val_end_idx = hyp_val_idx[key][1],\n",
    "                            metric_func= metric_function,\n",
    "                            min_training_episodes = 1, \n",
    "                            early_stop = True,\n",
    "                            stop_metric = metric,\n",
    "                            stop_patience = 3,\n",
    "                            stop_delta = 0.001)\n",
    "                rl_env.remove_agent(agent_name)\n",
    "\n",
    "        # Test Model\n",
    "        \n",
    "        scores = []\n",
    "        for key, rl_env in env.items():\n",
    "        \n",
    "            if key in tst_keys:\n",
    "                rl_env.add_agent(agent_name)\n",
    "                rl_env.set_decision_agent(agent_name)\n",
    "                if import_tn_csvs:\n",
    "                    timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')    #Requires outside variable              \n",
    "                best_agent.set_environment(rl_env)              \n",
    "                best_agent.test(start_idx = hyp_tst_idx[key][0],\n",
    "                            end_idx = hyp_tst_idx[key][1],\n",
    "                            metric_func= metric_function, \n",
    "                            testing_episodes=1)\n",
    "                rl_env.remove_agent(agent_name)\n",
    "\n",
    "                ## Save Test Metric Result(s) into \n",
    "                ddqn_tst = best_agent.get_testing_episodic_data()\n",
    "                score = ddqn_tst['tot_r'].mean()\n",
    "                scores.append(score)\n",
    "        \n",
    "                ## Export Test data\n",
    "                a = rl_env.get_step_data()\n",
    "                b = best_agent.get_step_data()\n",
    "                combined_df = pd.concat([a,b],axis=1)\n",
    "                tst_df_file_name  = f'TST-{key}' + test_name + '.csv'\n",
    "                trn_df_save_path = test_name_path + '/' + tst_df_file_name\n",
    "                combined_df.to_csv(trn_df_save_path)\n",
    "\n",
    "                ## Generate Trading Graphic\n",
    "                tst_graph_file_name = trn_df_save_path[:-4] + '.png'\n",
    "                agentperform.agent_stock_performance(env[key].stock_price_data[hyp_tst_idx[key][0]:hyp_tst_idx[key][1]][:,-1,0], # Selecting all batches, last price of window, closing price\n",
    "                                                    combined_df['Env Action'].to_numpy(),\n",
    "                                                    key,\n",
    "                                                    best_agent.get_name(),\n",
    "                                                    display_graph=False,\n",
    "                                                    save_graphic=True,\n",
    "                                                    path_file=tst_graph_file_name)\n",
    "\n",
    "        mean = np.mean(scores)\n",
    "        return mean\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"Best value: \", study.best_value)\n",
    "    print(\"Best params: \", study.best_params)\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Type Setup\n",
    "agent_classes = {'profit': DdqnAgent,\n",
    "                 'risk': DdqnAgent,\n",
    "                 'random':RandomAgent}\n",
    "\n",
    "# Mul\n",
    "agent_setup = {'profit': ['profit'],\n",
    "                 'risk': ['risk'],\n",
    "                 'random': ['random']}\n",
    "                 #final': ['profit', 'risk'], for multi agent key is decision agent\n",
    "                 #'macro': 'macro', \n",
    "                 #'opt': ['profit', 'risk', 'macro']}\n",
    "                 \n",
    "agent_name_list = list(agent_classes.keys())\n",
    "\n",
    "agents_to_train = ['profit', 'risk']\n",
    "agents_to_import = {'agent_name': 'path_to_model'}\n",
    "\n",
    "agent_params = {\n",
    "    agent_name_list[0]:{\n",
    "        'name': agent_name_list[0],\n",
    "        'environment': None,\n",
    "        'reward_function': future_profit,\n",
    "        'reward_params': {'n':5},\n",
    "        'env_state_mod_func': env_mod_func,\n",
    "        'input_size': 11,\n",
    "        'hidden_size': 256,\n",
    "        'output_size':3,\n",
    "        'activation_function': nn.Tanh(),\n",
    "        'num_hidden_layers': 2,\n",
    "        'buffer_size': 150,\n",
    "        'batch_size': 30,\n",
    "        'alpha': 0.005,\n",
    "        'gamma':0.97,\n",
    "        'opt_wgt_dcy': 0.001,\n",
    "        'dropout_rate': 0.35,\n",
    "        'device': device\n",
    "    },\n",
    "    agent_name_list[1]:{\n",
    "        'name': agent_name_list[1],\n",
    "        'environment': None,\n",
    "        'reward_function': risk_reward,\n",
    "        'reward_params': {'n':5},\n",
    "        'env_state_mod_func': env_mod_func,\n",
    "        'input_size': 11,\n",
    "        'hidden_size': 256,\n",
    "        'output_size':3,\n",
    "        'activation_function': nn.LeakyReLU(),\n",
    "        'num_hidden_layers': 2,\n",
    "        'buffer_size': 150,\n",
    "        'batch_size': 30,\n",
    "        'alpha': 0.005,\n",
    "        'gamma':0.97,\n",
    "        'opt_wgt_dcy': 0.001,\n",
    "        'dropout_rate': 0.35,\n",
    "        'device': device\n",
    "    },\n",
    "        agent_name_list[2]:{\n",
    "        'name': agent_name_list[2],\n",
    "        'environment': None,\n",
    "        'reward_function': zero_reward,\n",
    "        'reward_params': {},\n",
    "        }}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_dic = {}\n",
    "\n",
    "for agent_name, agent_class in agent_classes.items():\n",
    "            selected_agent = agent_class(**agent_params[agent_name])\n",
    "            agents_dic[agent_name] = selected_agent\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Inputs\n",
    "training_range = ('2007-01-01','2020-12-31')\n",
    "trn_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in training_range]\n",
    "\n",
    "# Validation Inputs\n",
    "validation_range = ('2021-01-01', '2021-12-31')\n",
    "val_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in validation_range]\n",
    "\n",
    "# Testing Inputs\n",
    "testing_range = ('2021-01-01', '2023-12-31')\n",
    "tst_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in testing_range]\n",
    "\n",
    "trn_idx = {}\n",
    "val_idx = {}\n",
    "tst_idx = {}\n",
    "\n",
    "for stock, file in stock_inputs.items():\n",
    "    if stock in set(trn_keys + val_keys + tst_keys):\n",
    "        if stock in trn_keys:\n",
    "            trn_idx[stock] = env_data[stock].gen_rw_idxs(trn_dt_range)\n",
    "        if stock in val_keys:\n",
    "            val_idx[stock] = env_data[stock].gen_rw_idxs(val_dt_range)\n",
    "        if stock in tst_keys:\n",
    "            tst_idx[stock] = env_data[stock].gen_rw_idxs(tst_dt_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {'training_episodes': 15,\n",
    "                   'epsilon_decya_func': linear_decay,\n",
    "                   'initial_epsilon': 0.9,\n",
    "                   'final_epsilon': 0.1,\n",
    "                   'update_q_freq': 1,\n",
    "                   'update_tgt_freq': 3,\n",
    "                   'save_path': export_path,\n",
    "                   'metric_func': metric_function,\n",
    "                   'min_training_episodes': 5,\n",
    "                   'early_stop': True,\n",
    "                   'stop_metric': 'val_tot_r',\n",
    "                   'stop_patience': 3,\n",
    "                   'stop_delta': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_agent:\n",
    "    filtered_agents = {\n",
    "        decision_agent: agents_in_setup\n",
    "        for decision_agent, agents_in_setup in agent_setup.items()\n",
    "        if decision_agent in agents_to_train\n",
    "    }\n",
    "\n",
    "    for decision_agent, agents_in_setup in filtered_agents.items():\n",
    "        for key in trn_keys:\n",
    "            rl_env = env[key]\n",
    "        \n",
    "            # Using Csvs\n",
    "            if import_tn_csvs:\n",
    "                timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')\n",
    "            \n",
    "            # Setup agents with environment  \n",
    "            for agent in agents_in_setup:\n",
    "                rl_env.add_agent(agent)\n",
    "                agents_dic[agent].set_environment(rl_env)\n",
    "            rl_env.set_decision_agent(decision_agent)\n",
    "            \n",
    "            # Train Sub-subagents\n",
    "            for agent in agents_in_setup:\n",
    "                if agent is not decision_agent:\n",
    "                    agents_dic[agent].train(start_idx=trn_idx[key][0],\n",
    "                                            end_idx=trn_idx[key][1],\n",
    "                                            val_start_idx= val_idx[key][0],\n",
    "                                            val_end_idx=val_idx[key][1],                                    \n",
    "                                            **training_params)\n",
    "            \n",
    "            # Train Decision Agent\n",
    "            agents_dic[decision_agent].train(start_idx=trn_idx[key][0],\n",
    "                                        end_idx=trn_idx[key][1],\n",
    "                                        val_start_idx= val_idx[key][0],\n",
    "                                        val_end_idx=val_idx[key][1],                                    \n",
    "                                        **training_params)\n",
    "            \n",
    "            # Remove Agent\n",
    "            for agent in agents_in_setup:\n",
    "                rl_env.remove_agent(agent)\n",
    "                agents_dic[agent].set_environment(None)\n",
    "        \"\"\"\n",
    "        ## Export Training Session Data to CSV\n",
    "        ddqn_trn = best_ddqn_agent.get_training_episodic_data()\n",
    "        ddqn_trn.to_csv('test.csv')\n",
    "        display(ddqn_trn)\n",
    "        env[key].remove_agent(best_ddqn_agent.get_name())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_params = {DdqnAgent: {\n",
    "                   'metric_func': metric_function,\n",
    "                   'metric_func_arg': {},\n",
    "                   'testing_episodes':1},\n",
    "                  RandomAgent: {\n",
    "                    'metric_func': metric_function,\n",
    "                   'metric_func_arg': {},\n",
    "                   'testing_episodes':100}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_agent:\n",
    "    result_dic_struct = ['stock','agent','test_interval','test_num']\n",
    "    results = {}\n",
    "\n",
    "    for key in tst_keys:\n",
    "        \n",
    "        # Init Record[Stock]\n",
    "        results[key] = {}\n",
    "        test_key = f'{tst_idx[key][0]}:{tst_idx[key][1]}'\n",
    "        stock_price_data = env_data[key]['rw_raw_price_env'][tst_idx[key][0]:tst_idx[key][1],-1,0]\n",
    "        rl_env = env[key]\n",
    "\n",
    "        for decision_agent, agents_in_setup in agent_setup.items():\n",
    "\n",
    "            # Init Record[Stock][Agent]\n",
    "            results[key][decision_agent] = {}\n",
    "            \n",
    "            # Init Record[Stock][Agent][test_interval]\n",
    "            results[key][decision_agent][test_key] = {}   # Different Test Keys will need loop\n",
    "            \n",
    "            # Using Csvs       \n",
    "            if import_tn_csvs:\n",
    "                timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')\n",
    "            \n",
    "            # Setup agents with environment  \n",
    "            for agent in set([decision_agent] + agents_in_setup):\n",
    "                rl_env.add_agent(agent)\n",
    "                agents_dic[agent].set_environment(rl_env)\n",
    "            rl_env.set_decision_agent(decision_agent)\n",
    "            \n",
    "            # Enable Randomess if Agent is of class RandomAgent\n",
    "            if isinstance(agents_dic[decision_agent], RandomAgent):\n",
    "                new_random_seed = random.randint(1, 10**9)\n",
    "                set_seed(new_random_seed)\n",
    "                \n",
    "            # Test Decision Agent\n",
    "            params = testing_params[agent_classes[decision_agent]]\n",
    "            agents_dic[decision_agent].test(start_idx=tst_idx[key][0],\n",
    "                                            end_idx=tst_idx[key][1],\n",
    "                                            **params)\n",
    "            test_results = agents_dic[decision_agent].get_testing_episodic_data()\n",
    "            trade_actions_per_test = test_results['tst_actions']\n",
    "                \n",
    "            for idx, action_set in enumerate(trade_actions_per_test):\n",
    "                test_metrics = agentperform.agent_stock_performance(stock_price_ts=np.array(stock_price_data),\n",
    "                                                                    trade_ts=np.array(action_set),\n",
    "                                                                    stock_name=key,\n",
    "                                                                    agent_name=decision_agent,\n",
    "                                                                    display_graph=False, \n",
    "                                                                    save_graphic= False,\n",
    "                                                                    path_file = None)\n",
    "                del test_metrics['stock']\n",
    "                del test_metrics['agent_name']                                                                     \n",
    "                results[key][decision_agent][test_key][idx] = test_metrics\n",
    "            \n",
    "            # Remove Agent\n",
    "            for agent in set([decision_agent] + agents_in_setup):\n",
    "                rl_env.remove_agent(agent)\n",
    "                agents_dic[agent].set_environment(None)\n",
    "\n",
    "    display(results)\n",
    "    set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggreating Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_agent:\n",
    "    aggerate_results = {}\n",
    "    for agent in agent_name_list:\n",
    "        aggerate_results[agent] = {}\n",
    "        for stock in tst_keys:\n",
    "            aggerate_results[agent][stock] = {}\n",
    "            test_key = f'{tst_idx[stock][0]}:{tst_idx[stock][1]}'\n",
    "            aggerate_results[agent][stock][test_key] = {}\n",
    "            values = np.empty((0,len(metrics)))\n",
    "            for test_num in range(testing_params[agent_classes[agent]]['testing_episodes']):\n",
    "\n",
    "                values_array = [results[stock][agent][test_key][test_num][key] for key in metrics]\n",
    "                current_values = np.array(values_array)\n",
    "                values = np.vstack((values,current_values))\n",
    "\n",
    "                means_for_metrics = np.mean(values, axis=0)\n",
    "                std_for_metrics = np.std(values, axis=0)\n",
    "            \n",
    "            for idx,metric in enumerate(metrics):\n",
    "                aggerate_results[agent][stock][test_key][metric] = (means_for_metrics[idx],std_for_metrics[idx])\n",
    "\n",
    "    summarized_aggerate_results = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        model_list = []\n",
    "        dataset_name = []\n",
    "        scores = []\n",
    "        for agent in aggerate_results.keys():\n",
    "            \n",
    "            model_list.append(agent)\n",
    "            score_list = []\n",
    "            for stock in aggerate_results[agent].keys():\n",
    "                for test in aggerate_results[agent][stock].keys():\n",
    "                    run_name = stock + \"-\" + test\n",
    "                    if run_name not in dataset_name:\n",
    "                        dataset_name.append(run_name)\n",
    "                    score = aggerate_results[agent][stock][test][metric][0]\n",
    "                    score_list.append(np.round(score,2))\n",
    "            scores.append(score_list)\n",
    "\n",
    "        score_array = np.array(scores).T\n",
    "\n",
    "        df = pd.DataFrame(score_array,columns=model_list)\n",
    "        df['dataset'] = dataset_name\n",
    "\n",
    "        column_order = ['dataset'] + [col for col in df.columns if col != 'dataset']\n",
    "        df = df[column_order]\n",
    "\n",
    "        model_means = list(zip(model_list,df[model_list].mean()))\n",
    "\n",
    "        summarized_aggerate_results[metric] = (df, model_means)\n",
    "\n",
    "\n",
    "    display(summarized_aggerate_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_agent:\n",
    "    for metric in metrics:\n",
    "        display(metric)\n",
    "        display(summarized_aggerate_results[metric][0])\n",
    "        test = prob_evaluate.generate_rank_array_from_dataframe(summarized_aggerate_results[metric][0],\n",
    "                                                                model_list,equal_rank_behav=\"mean\",\n",
    "                                                                rank_order=aval_metrics_rank_dic[metric])\n",
    "        display(test)\n",
    "        stat, critical_f_value, reject_null_hypo = prob_evaluate.iman_davenport_test(test,0.95,arr_order='rows')\n",
    "        display(stat, critical_f_value, reject_null_hypo)\n",
    "\n",
    "        results1 = prob_evaluate.nemenyi_test(test,0.95,model_list)\n",
    "        display(results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ?Final Hypertuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Objective function, need to create agent name before to link agent with enviornment\n",
    "agent_name = 'REWARD_DDQN_AGENT'\n",
    "agent_path = export_path + '/' + agent_name\n",
    "metric = 'val_ror'\n",
    "\n",
    "for key, env in environments.items():\n",
    "  \n",
    "        env.add_agent(agent_name)\n",
    "        env.set_decision_agent(agent_name)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Define the hyperparameters to search over\n",
    "    \n",
    "    ## NN hyperparameters\n",
    "    sug_hidden_layers = trial.suggest_int('hidden_layers', low=1, high=3)\n",
    "    sug_hidden_size = trial.suggest_int('hidden_size', low=256, high=1280, step=64)\n",
    "    sug_update_q_freq = trial.suggest_int('update_q_freq',low=1,high=5)\n",
    "    sug_update_tgt_freq = trial.suggest_int('update_tgt_freq',low=10,high=50,step=10)\n",
    "    \n",
    "    ## Function Passing\n",
    "    activation_functions = {\n",
    "    'RELU': nn.ReLU(),\n",
    "    'LRELU': nn.LeakyReLU(),\n",
    "    'GELU': nn.GELU(),\n",
    "    'TANH': nn.Tanh()\n",
    "    }\n",
    "    sug_activation_function_name = trial.suggest_categorical('activation_function', list(activation_functions.keys()))\n",
    "    sug_activation_function = activation_functions[sug_activation_function_name]\n",
    "    \n",
    "    ## Optimizer hyperparameters\n",
    "    sug_opt_lre = trial.suggest_float('opt_lre',0.0001,0.1,log=True)\n",
    "    ## Memory Replay hyperparameters\n",
    "    sug_buffer_size = trial.suggest_int('buffer_size',low=100,high=1500,step=100)\n",
    "    sug_batch_size = trial.suggest_int('batch_size',low=10,high=150,step=10)\n",
    "\n",
    "    # Saving Setup\n",
    "    ## Current Parameter Values:\n",
    "    cur_n_fcl = trial.params['hidden_layers']\n",
    "    cur_fcl_size = trial.params['hidden_size']\n",
    "    cur_q_freq = trial.params['update_q_freq']\n",
    "    cur_tgt_freq = trial.params['update_tgt_freq']\n",
    "    cur_act_func = trial.params['activation_function']\n",
    "    cur_lre = decimal_to_text(trial.params['opt_lre'])\n",
    "    cur_buf_size = trial.params['buffer_size']\n",
    "    cur_bat_size = trial.params['batch_size']\n",
    "    \n",
    "    ## Create Notation for Hyperparameter Setup    \n",
    "    test_name = (f'{cur_n_fcl}FC{cur_fcl_size}_{cur_act_func}_' +\n",
    "                f'BT{cur_bat_size}BF{cur_buf_size}_Q{cur_q_freq}_' +\n",
    "                f'TGT{cur_tgt_freq}_LR{cur_lre}')\n",
    "    \n",
    "    ## Create Dir to save results\n",
    "    test_name_path =  agent_path + '/' + test_name \n",
    "    if not os.path.exists(test_name_path):\n",
    "        os.makedirs(test_name_path)\n",
    "        print(f\"Directory '{test_name_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory '{test_name_path}' already exists.\")    \n",
    "    \n",
    "    # Create Agent with hyperparameters  \n",
    "    best_ddqn_agent = DdqnAgent(name=agent_name,\n",
    "                        environment=None,\n",
    "                        reward_function = future_profit,\n",
    "                        reward_params = {'n':5},\n",
    "                        env_state_mod_func = flatten_state,     \n",
    "                        input_size= 13,\n",
    "                        hidden_size= sug_hidden_size, \n",
    "                        output_size=3, \n",
    "                        activation_function = sug_activation_function,\n",
    "                        num_hidden_layers = sug_hidden_layers,                  \n",
    "                        buffer_size= sug_buffer_size, \n",
    "                        batch_size = sug_batch_size,\n",
    "                        opt_lr= sug_opt_lre,\n",
    "                        alpha = ALPHA,\n",
    "                        gamma = GAMMA,\n",
    "                        opt_wgt_dcy = 0.0,\n",
    "                        dropout_rate = 0.25,                \n",
    "                        device = device)\n",
    "\n",
    "    # Training Model\n",
    "    for key, env in environments.items():\n",
    "        \n",
    "        if key in trn_keys:\n",
    "            \n",
    "            best_ddqn_agent.set_environment(env)\n",
    "            best_ddqn_agent.train(start_idx=training_range[0],\n",
    "                        end_idx=training_range[1],\n",
    "                        training_episodes= 100,\n",
    "                        epsilon_decya_func= linear_decay,\n",
    "                        initial_epsilon= 0.9,\n",
    "                        final_epsilon= 0.1,\n",
    "                        update_q_freq= sug_update_q_freq,\n",
    "                        update_tgt_freq= sug_update_tgt_freq,\n",
    "                        save_path = export_path,\n",
    "                        val_start_idx = validation_range[0],\n",
    "                        val_end_idx = validation_range[1],\n",
    "                        early_stop = True,\n",
    "                        stop_metric = metric,\n",
    "                        stop_patience = 20,\n",
    "                        stop_delta = 0.001)\n",
    "        \n",
    "            ## Export Training Session Data to CSV\n",
    "            ddqn_trn = best_ddqn_agent.get_training_episodic_data()\n",
    "            trn_df_file_name  = f'TRN-{key}' + test_name + '.csv'\n",
    "            trn_df_save_path = test_name_path + '/' + trn_df_file_name\n",
    "            ddqn_trn.to_csv(trn_df_save_path)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Test Model\n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "    for key, env in environments.items():\n",
    "    \n",
    "        if key in tst_keys:\n",
    "            \n",
    "            best_ddqn_agent.set_environment(env)              \n",
    "            best_ddqn_agent.test(start_idx = testing_range[0],\n",
    "                        end_idx = testing_range[1], \n",
    "                        testing_episodes=1)\n",
    "\n",
    "            ## Save Test Metric Result(s) into \n",
    "            ddqn_tst = best_ddqn_agent.get_testing_episodic_data()\n",
    "            score = ddqn_tst['Total Reward'].mean()\n",
    "            scores.append(score)\n",
    "    \n",
    "            ## Export Test data\n",
    "            a = env.get_step_data()\n",
    "            b = best_ddqn_agent.get_step_data()\n",
    "            combined_df = pd.concat([a,b],axis=1)\n",
    "            tst_df_file_name  = f'TST-{key}' + test_name + '.csv'\n",
    "            trn_df_save_path = test_name_path + '/' + tst_df_file_name\n",
    "            combined_df.to_csv(trn_df_save_path)\n",
    "\n",
    "            ## Generate Trading Graphic\n",
    "            tst_graph_file_name = trn_df_save_path[:-4] + '.png'\n",
    "            agentperform.agent_stock_performance(env.stock_price_data[testing_range[0]:testing_range[1]],\n",
    "                                                combined_df['Env Action'].to_numpy(),\n",
    "                                                key,\n",
    "                                                best_ddqn_agent.get_name(),\n",
    "                                                display_graph=True,\n",
    "                                                save_graphic=True,\n",
    "                                                path_file=tst_graph_file_name)\n",
    "\n",
    "    mean = np.mean(scores)\n",
    "    return mean\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best value: \", study.best_value)\n",
    "print(\"Best params: \", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MADDQN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
