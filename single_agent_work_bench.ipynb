{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Code specific to Google Colab\n",
    "    print(\"Running in Google Colab\")\n",
    "\n",
    "    # Additional setup commands for Colab\n",
    "    !pip install neuralforecast\n",
    "    !pip install gymnasium\n",
    "    !pip install QuantStats\n",
    "else:\n",
    "    # Code for other environments (e.g., VS Code)\n",
    "    print(\"Running in another environment (e.g., VS Code)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install RL Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Retrive required files\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/environments/stockenv.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/cleandata.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/data.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/epsilon_decay.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/utilities/agentperform.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/ddqn.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/random.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/agents/baseagent.py\n",
    "    !wget --recursive --no-parent --cut-dirs=4 -P /content https://raw.githubusercontent.com//CodeBeckZero/MADDQN/main/rewards/stockmarket.py            \n",
    "    # Move all directories and files from content/raw.githubusercontent.com to content/\n",
    "    !mv /content/raw.githubusercontent.com/* /content/\n",
    "\n",
    "    # Delete the raw.githubusercontent.com directory\n",
    "    !rm -rf /content/raw.githubusercontent.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activate Python Libraries & Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import optuna\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import utilities.agentperform as agentperform\n",
    "import utilities.cleandata as cln \n",
    "from utilities.epsilon_decay import linear_decay\n",
    "from utilities.data import UniStockEnvDataStruct, TimesNetProcessing\n",
    "from agents.ddqn import DdqnAgent\n",
    "from rewards.stockmarket import future_profit, risk_reward\n",
    "from environments.stockenv import ContinuousOHLCVEnv\n",
    "from datetime import datetime\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import TimesNet\n",
    "from neuralforecast.losses.numpy import mae, mse\n",
    "import logging\n",
    "\n",
    "\n",
    "# \n",
    "logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").addHandler(logging.NullHandler())\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").addHandler(logging.NullHandler())\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1' # Prevent Warning \n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    # Python random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # If you are using CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "        # Additional settings to force determinism in your operations:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"CUDA (GPU support) is not available. PyTorch is running on CPU.\")\n",
    "\n",
    "\n",
    "def decimal_to_text(decimal_number):\n",
    "    # Remove the decimal point and convert to integer\n",
    "    integer_part = int(decimal_number * 1000)\n",
    "    # Convert the integer to text\n",
    "    text_representation = str(integer_part)\n",
    "    return text_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters & CSV Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "if not IN_COLAB:\n",
    "    pwd = \"C:/programming/MADDQN\"\n",
    "    sys.path.append(pwd)\n",
    "    \n",
    "    # Output Path Location for CSV export\n",
    "    export_path = pwd + \"/output_data\"\n",
    "\n",
    "# Input Data Location, File Name, Stock name for labels\n",
    "input_url = 'https://raw.githubusercontent.com/CodeBeckZero/MADDQN/main/input_data'\n",
    "\n",
    "stock_inputs ={'DJI':'^DJI_daily.csv',\n",
    "               'NDAQ': '^IXIC_daily.csv',\n",
    "               'SP500': '^SPX_daily.csv',\n",
    "               'AAPL': 'AAPL_daily.csv',\n",
    "               'AMZN': 'AMZN_daily.csv',\n",
    "               'GOOGL': 'GOOGL_daily.csv',\n",
    "               'MSFT': 'MSFT_daily.csv',\n",
    "               'SINE': 'sine_wave_daily.csv',\n",
    "               'FORD': 'F_daily.csv',\n",
    "               'JNJ': 'JNJ_daily.csv',\n",
    "               'NEE': 'NEE_daily.csv',\n",
    "               'PFE': 'PFE_daily.csv',\n",
    "               'TSLA': 'TSLA_daily.csv',\n",
    "               'COKE': 'COKE_daily.csv',\n",
    "               'PG': 'PG_daily.csv'}\n",
    "\n",
    "# Training Inputs\n",
    "trn_keys = ['DJI','NDAQ','SP500']\n",
    "training_range = ('2007-01-01','2020-12-31')\n",
    "trn_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in training_range]\n",
    "\n",
    "# Validation Inputs\n",
    "val_keys = trn_keys\n",
    "validation_range = ('2021-01-01', '2021-12-31')\n",
    "val_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in validation_range]\n",
    "\n",
    "# Testing Inputs\n",
    "tst_keys = ['AAPL','AMAZON','GOOGL','MSFT','FORD','JNJ','NEE','PFE','TSLA','COKE','PG']\n",
    "testing_range = ('2021-01-01', '2023-12-31')\n",
    "tst_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in testing_range]\n",
    "\n",
    "window_size = 28 # Needs to match the size Timesnet is trained on\n",
    "price_based_on = 'close'\n",
    "columns = ['open','high','low','close','volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Enviornment Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_data = {}\n",
    "env = {}\n",
    "\n",
    "for stock, file in stock_inputs.items():\n",
    "    if stock in set(trn_keys + val_keys + tst_keys):\n",
    "        # Import\n",
    "        df = cln.YAHOO_csv_input(file, input_url)\n",
    "        data_dic = UniStockEnvDataStruct(df,price_based_on,window_size)\n",
    "        env_data[stock] = data_dic\n",
    "        env[stock] = ContinuousOHLCVEnv(name=stock,\n",
    "                                        ohlcv_data = env_data[stock]['rw_raw_env'] ,\n",
    "                                        stock_price_data= env_data[stock]['rw_raw_price_env'],\n",
    "                                        commission_rate=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbench Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_discovery = True                 # Limited Exploratory Hyperparmater Discover for RL Agent\n",
    "n_prediction = 5                            # Number of price predictions in the Future by TimesNet (Required for RL agent's input layer)\n",
    "train_tn_model = False                      # Need to Train TimesNet Preprocessing model (processed every cycle)\n",
    "import_tn_model = False                     # Importing TimesNet Preprocessing model (processed every cycle)\n",
    "import_tn_csvs = True                       # Use Imported CSVs from Preprocessing model (no processing, straight to RL agent)\n",
    "tn_path = pwd + '/gen_data/timesnet/'       # Location of\n",
    "no_tn_preprocessing = False                 #\n",
    "\n",
    "timesnet = TimesNetProcessing(env_data)\n",
    "\n",
    "env_mod_func_dic = {'train': timesnet.process,\n",
    "               'import':timesnet.process,\n",
    "               'csv':timesnet.csv_process,\n",
    "               'none': None}\n",
    "\n",
    "if  train_tn_model ^ import_tn_model ^ import_tn_csvs ^ no_tn_preprocessing:\n",
    "    if train_tn_model or import_tn_model:\n",
    "        env_mod_func = env_mod_func_dic['train']\n",
    "    if import_tn_csvs:\n",
    "        env_mod_func = env_mod_func_dic['csv']\n",
    "    if no_tn_preprocessing:\n",
    "        env_mod_func = env_mod_func_dic['none']\n",
    "else:\n",
    "    raise ValueError(\"Only one TimesNet Preprocessing Options can be selected\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_function(env):\n",
    "    metric = env.step_info[-1]['New Portfolio Value'] -  env.step_info[-1]['Portfolio Value']\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimesNet Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_tn_model:\n",
    "    \n",
    "    model = TimesNet(h = n_prediction,          # Forecast horizon\n",
    "                    input_size = window_size,   # Length of Batches\n",
    "                    batch_size = 1,             # Number of timeseries to predict\n",
    "                    #futr_exog_list = remaining_columns,\n",
    "                    hidden_size = 128,          # Size of embedding for embedding and encoders,\n",
    "                    dropout = 0.40,             # Dropout for embeddings\n",
    "                    conv_hidden_size = 3,       # Channels for the inception block\n",
    "                    top_k = 5,                  # Top num of periods from FFT considered\n",
    "                    num_kernels = 13,           # number of kernels for the inception block\n",
    "                    encoder_layers = 3,         # num of encoders\n",
    "                    max_steps = 1000,           # of training steps\n",
    "                    early_stop_patience_steps = 10, #early stoppage on validation\n",
    "                    val_check_steps = 100,      # Val check every X steps,\n",
    "                    windows_batch_size = 150,   # Number of windows in training epoch,\n",
    "                    num_workers_loader = 7,\n",
    "                    learning_rate = 0.0003,\n",
    "                    random_seed = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_tn_model:\n",
    "  nf = NeuralForecast(models=[model], freq='d')\n",
    "  results = {}\n",
    "  for key in trn_keys:\n",
    "    results[key] = nf.fit(df=env[key],val_size=0.2)\n",
    "\n",
    "  nf.save(path= tn_path,\n",
    "          model_index=None,\n",
    "          overwrite=True,\n",
    "          save_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_tn_model:\n",
    "# Define the correct path\n",
    "  if IN_COLAB:\n",
    "    \n",
    "    model_path = os.path.join(os.getcwd(), 'gen_data', 'timesnet')\n",
    "\n",
    "    # Ensure the directory and file exist\n",
    "    if os.path.exists(model_path):\n",
    "        nf = NeuralForecast.load(path=model_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model path {model_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_tn_csvs:\n",
    "\n",
    "    if IN_COLAB:\n",
    "        # Input Data Location, File Name, Stock name for labels\n",
    "        csv_path = 'https://raw.githubusercontent.com/CodeBeckZero/MADDQN/main/gen_data/csvs/'\n",
    "\n",
    "    else:\n",
    "        csv_path  = pwd +'/gen_data/csvs/'\n",
    "\n",
    "    stock_tn ={'DJI':'DJI_tn.csv',\n",
    "                'NDAQ': 'NDAQ_tn.csv',\n",
    "                'SP500': 'SP500_tn.csv',\n",
    "                'AAPL': 'AAPL_tn.csv',\n",
    "                'AMZN': 'AMZN_tn.csv',\n",
    "                'GOOGL': 'GOOGL_tn.csv',\n",
    "                'MSFT': 'MSFT_tn.csv',\n",
    "                'FORD': 'FORD_tn.csv',\n",
    "                'JNJ': 'JNJ_tn.csv',\n",
    "                'NEE': 'NEE_tn.csv',\n",
    "                'PFE': 'PFE_tn.csv',\n",
    "                'TSLA': 'TSLA_tn.csv',\n",
    "                'COKE': 'COKE_tn.csv',\n",
    "                'PG': 'PG_tn.csv'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Hyperparameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparam_discovery:\n",
    "    # Training Inputs\n",
    "    hyp_training_range = ('2007-01-01','2010-12-31')\n",
    "    hyp_trn_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_training_range]\n",
    "\n",
    "    # Validation Inputs\n",
    "    hyp_validation_range = ('2013-01-01', '2014-12-31')\n",
    "    hyp_val_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_validation_range]\n",
    "\n",
    "    # Testing Inputs\n",
    "    hyp_testing_range = ('2016-01-01', '2017-12-31')\n",
    "    hyp_tst_dt_range = [datetime.strptime(dt_str, \"%Y-%m-%d\") for dt_str in hyp_testing_range]\n",
    "\n",
    "    hyp_trn_idx = {}\n",
    "    hyp_val_idx = {}\n",
    "    hyp_tst_idx = {}\n",
    "\n",
    "    for stock, file in stock_inputs.items():\n",
    "        if stock in set(trn_keys + val_keys + tst_keys):\n",
    "            if stock in trn_keys:\n",
    "                hyp_trn_idx[stock] = env_data[stock].gen_rw_idxs(hyp_trn_dt_range)\n",
    "            if stock in val_keys:\n",
    "                hyp_val_idx[stock] = env_data[stock].gen_rw_idxs(hyp_val_dt_range)\n",
    "            if stock in tst_keys:\n",
    "                hyp_tst_idx[stock] = env_data[stock].gen_rw_idxs(hyp_tst_dt_range)\n",
    "\n",
    "display(hyp_trn_idx,hyp_val_idx,hyp_tst_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Search & Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparam_discovery:\n",
    "    \n",
    "    # For Objective function, need to create agent name before to link agent with enviornment\n",
    "    agent_name = 'hyp_discovery_agent'\n",
    "    agent_path = export_path + '/' + agent_name\n",
    "    metric = 'val_tot_r'\n",
    "    max_len_buf = np.round(hyp_trn_idx['DJI'][1] - hyp_trn_idx['DJI'][0] + window_size, -2) -10 # manual input, could be error here if \n",
    "    print(f'Max Mem Length: {max_len_buf}')\n",
    "           \n",
    "    def objective(trial):\n",
    "    \n",
    "        # Define the hyperparameters to search over\n",
    "        \n",
    "        ## NN hyperparameters\n",
    "        sug_hidden_layers = trial.suggest_int('hidden_layers', low=1, high=3)\n",
    "        sug_hidden_size = trial.suggest_int('hidden_size', low=64, high=512, step=64)\n",
    "        sug_update_q_freq = trial.suggest_int('update_q_freq',low=1,high=5)\n",
    "        sug_update_tgt_freq = trial.suggest_int('update_tgt_freq',low=5,high=15)\n",
    "        \n",
    "        ## Activation Function Passing\n",
    "        activation_functions = {\n",
    "        'LRELUd': nn.LeakyReLU(),\n",
    "        'LRELUs02': nn.LeakyReLU(negative_slope=0.2),\n",
    "        'GELU': nn.GELU(),\n",
    "        'TANH': nn.Tanh(),\n",
    "        'SELU':nn.SELU(),\n",
    "        'SILU': nn.SiLU()\n",
    "        }\n",
    "        sug_activation_function_name = trial.suggest_categorical('activation_function', list(activation_functions.keys()))\n",
    "        sug_activation_function = activation_functions[sug_activation_function_name]\n",
    "        \n",
    "        \"\"\"\n",
    "        ## Reward Function Passing\n",
    "        reward_functions = {\n",
    "        'profit': future_profit(None,5),\n",
    "        'risk': risk_reward(None,5),\n",
    "        }\n",
    "        sug_reward_function_name = trial.suggest_categorical('reward_function', list(reward_functions.keys()))\n",
    "        sug_reward_function = reward_functions[sug_reward_function_name]\n",
    "        \"\"\"\n",
    "        ## Optimizer hyperparameters\n",
    "        sug_opt_lre = trial.suggest_categorical('opt_lre',[0.0001,0.0005,0.001, 0.005, 0.01, 0.05, 0.1])\n",
    "        sug_gamma = trial.suggest_float('gamma',low=0.90,high=0.99,step=0.01)\n",
    "        ## Memory Replay hyperparameters\n",
    "        sug_buffer_size = trial.suggest_int('buffer_size',low=100,high=max_len_buf,step=10)\n",
    "        sug_batch_size = trial.suggest_int('batch_size',low=10,high=sug_buffer_size,step=5)\n",
    "        \n",
    "        # Saving Setup\n",
    "        ## Current Parameter Values:\n",
    "        cur_n_fcl = trial.params['hidden_layers']\n",
    "        cur_fcl_size = trial.params['hidden_size']\n",
    "        cur_q_freq = trial.params['update_q_freq']\n",
    "        cur_tgt_freq = trial.params['update_tgt_freq']\n",
    "        cur_act_func = trial.params['activation_function']\n",
    "        #cur_rwd_func = trial.params['reward_function']\n",
    "        cur_lre = decimal_to_text(trial.params['opt_lre'])\n",
    "        cur_buf_size = trial.params['buffer_size']\n",
    "        cur_bat_size = trial.params['batch_size']\n",
    "        \n",
    "        ## Create Notation for Hyperparameter Setup    \n",
    "        test_name = (f'{cur_n_fcl}FC{cur_fcl_size}_{cur_act_func}_' +\n",
    "                    f'BT{cur_bat_size}BF{cur_buf_size}_Q{cur_q_freq}_' +\n",
    "                    f'TGT{cur_tgt_freq}_LR{cur_lre}')\n",
    "        \n",
    "        ## Create Dir to save results\n",
    "        test_name_path =  agent_path + '/' + test_name \n",
    "        if not os.path.exists(test_name_path):\n",
    "            os.makedirs(test_name_path)\n",
    "            print(f\"Directory '{test_name_path}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Directory '{test_name_path}' already exists.\")\n",
    "        \n",
    "        # Create Agent with hyperparameters  \n",
    "        best_agent = DdqnAgent(name=agent_name,\n",
    "                            environment=None,\n",
    "                            reward_function = future_profit,\n",
    "                            reward_params = {'n':5},\n",
    "                            env_state_mod_func = env_mod_func,     \n",
    "                            input_size= 11,\n",
    "                            hidden_size= sug_hidden_size, \n",
    "                            output_size=3, \n",
    "                            activation_function = sug_activation_function,\n",
    "                            num_hidden_layers = sug_hidden_layers,                  \n",
    "                            buffer_size= sug_buffer_size, \n",
    "                            batch_size = sug_batch_size,\n",
    "                            alpha = sug_opt_lre,\n",
    "                            gamma = sug_gamma,\n",
    "                            opt_wgt_dcy = 0.01,\n",
    "                            dropout_rate = 0.25,                \n",
    "                            device = device)\n",
    "        \n",
    "        # Training Model\n",
    "        for key, rl_env in env.items():\n",
    "            \n",
    "            if key in trn_keys:\n",
    "                rl_env.add_agent(agent_name)\n",
    "                rl_env.set_decision_agent(agent_name)\n",
    "                if import_tn_csvs:\n",
    "                    timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')    #Requires outside variable         \n",
    "                best_agent.set_environment(rl_env)\n",
    "                best_agent.train(start_idx=hyp_trn_idx[key][0],\n",
    "                            end_idx=hyp_trn_idx[key][1],\n",
    "                            training_episodes= 1,\n",
    "                            epsilon_decya_func= linear_decay,\n",
    "                            initial_epsilon= 0.9,\n",
    "                            final_epsilon= 0.1,\n",
    "                            update_q_freq= sug_update_q_freq,\n",
    "                            update_tgt_freq= sug_update_tgt_freq,\n",
    "                            save_path = export_path,\n",
    "                            val_start_idx = hyp_val_idx[key][0],\n",
    "                            val_end_idx = hyp_val_idx[key][1],\n",
    "                            metric_func= metric_function,\n",
    "                            min_training_episodes = 1, \n",
    "                            early_stop = True,\n",
    "                            stop_metric = metric,\n",
    "                            stop_patience = 3,\n",
    "                            stop_delta = 0.001)\n",
    "                rl_env.remove_agent(agent_name)\n",
    "\n",
    "        # Test Model\n",
    "        \n",
    "        scores = []\n",
    "        for key, rl_env in env.items():\n",
    "        \n",
    "            if key in tst_keys:\n",
    "                rl_env.add_agent(agent_name)\n",
    "                rl_env.set_decision_agent(agent_name)\n",
    "                if import_tn_csvs:\n",
    "                    timesnet.upload_csv(f'{csv_path}/{stock_tn[key]}')    #Requires outside variable              \n",
    "                best_agent.set_environment(rl_env)              \n",
    "                best_agent.test(start_idx = hyp_tst_idx[key][0],\n",
    "                            end_idx = hyp_tst_idx[key][1],\n",
    "                            metric_func= metric_function, \n",
    "                            testing_episodes=1)\n",
    "                rl_env.remove_agent(agent_name)\n",
    "\n",
    "                ## Save Test Metric Result(s) into \n",
    "                ddqn_tst = best_agent.get_testing_episodic_data()\n",
    "                score = ddqn_tst['tot_r'].mean()\n",
    "                scores.append(score)\n",
    "        \n",
    "                ## Export Test data\n",
    "                a = rl_env.get_step_data()\n",
    "                b = best_agent.get_step_data()\n",
    "                combined_df = pd.concat([a,b],axis=1)\n",
    "                tst_df_file_name  = f'TST-{key}' + test_name + '.csv'\n",
    "                trn_df_save_path = test_name_path + '/' + tst_df_file_name\n",
    "                combined_df.to_csv(trn_df_save_path)\n",
    "\n",
    "                ## Generate Trading Graphic\n",
    "                tst_graph_file_name = trn_df_save_path[:-4] + '.png'\n",
    "                agentperform.agent_stock_performance(env[key].stock_price_data[hyp_tst_idx[key][0]:hyp_tst_idx[key][1]][:,-1,0], # Selecting all batches, last price of window, closing price\n",
    "                                                    combined_df['Env Action'].to_numpy(),\n",
    "                                                    key,\n",
    "                                                    best_agent.get_name(),\n",
    "                                                    display_graph=False,\n",
    "                                                    save_graphic=True,\n",
    "                                                    path_file=tst_graph_file_name)\n",
    "\n",
    "        mean = np.mean(scores)\n",
    "        return mean\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best value: \", study.best_value)\n",
    "print(\"Best params: \", study.best_params)\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Setup\n",
    "\n",
    "agent_classes = {'profit': DdqnAgent,\n",
    "                 'risk': DdqnAgent}\n",
    "\n",
    "agent_list = list(agent_classes.keys())\n",
    "\n",
    "agent_params = {\n",
    "    agent_list[0]:{\n",
    "        'name': agent_list[0],\n",
    "        'environment': None,\n",
    "        'reward_function': future_profit,\n",
    "        'reward_params': {'n':5},\n",
    "        'env_state_mod_func': env_mod_func['csv'],\n",
    "        'input_size': 11,\n",
    "        'hidden_size': 256,\n",
    "        'output_size':3,\n",
    "        'activation_function': nn.Tanh(),\n",
    "        'num_hidden_layers': 2,\n",
    "        'buffer_size': 150,\n",
    "        'batch_size': 30,\n",
    "        'alpha': 0.005,\n",
    "        'gamma':0.97,\n",
    "        'opt_wgt_dcy': 0.01,\n",
    "        'dropout_rate': 0.25,\n",
    "        'device' = device\n",
    "    },\n",
    "    agent_list[1]:{\n",
    "        'name': agent_list[1],\n",
    "        'environment': None,\n",
    "        'reward_function': risk_reward,\n",
    "        'reward_params': {'n':5},\n",
    "        'env_state_mod_func': env_mod_func['csv'],\n",
    "        'input_size': 11,\n",
    "        'hidden_size': 256,\n",
    "        'output_size':3,\n",
    "        'activation_function': nn.Tanh(),\n",
    "        'num_hidden_layers': 2,\n",
    "        'buffer_size': 150,\n",
    "        'batch_size': 30,\n",
    "        'alpha': 0.005,\n",
    "        'gamma':0.97,\n",
    "        'opt_wgt_dcy': 0.01,\n",
    "        'dropout_rate': 0.25,\n",
    "        'device' = device\n",
    "    }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_mod_func = {'train': timesnet.process,\n",
    "               'import':timesnet.process,\n",
    "               'csv':timesnet.csv_process,}\n",
    "\n",
    "\n",
    "best_ddqn_agent = DdqnAgent(name='profit',\n",
    "                        environment=None,\n",
    "                        reward_function = future_profit,\n",
    "                        reward_params = {'n':5},\n",
    "                        env_state_mod_func = env_mod_func['csv'],     \n",
    "                        input_size= 11,\n",
    "                        hidden_size= 256, \n",
    "                        output_size=3, \n",
    "                        activation_function = nn.Tanh(),\n",
    "                        num_hidden_layers = 2,                  \n",
    "                        buffer_size= 150, \n",
    "                        batch_size = 30,\n",
    "                        alpha = 0.005,\n",
    "                        gamma = 0.97,\n",
    "                        opt_wgt_dcy = 0.01,\n",
    "                        dropout_rate = 0.25,                \n",
    "                        device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_idx = {}\n",
    "val_idx = {}\n",
    "tst_idx = {}\n",
    "\n",
    "for stock, file in stock_inputs.items():\n",
    "    if stock in set(trn_keys + val_keys + tst_keys):\n",
    "        if stock in trn_keys:\n",
    "            trn_idx[stock] = env_data[stock].gen_rw_idxs(trn_dt_range)\n",
    "        if stock in val_keys:\n",
    "            val_idx[stock] = env_data[stock].gen_rw_idxs(val_dt_range)\n",
    "        if stock in tst_keys:\n",
    "            tst_idx[stock] = env_data[stock].gen_rw_idxs(tst_dt_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, rl_env in env.items():\n",
    "    \n",
    "    if key in trn_keys:\n",
    "        timesnet.upload_csv(csv_path + stock_tn[key])\n",
    "        print(env[key].agent_sequence,env[key].DECISION_AGENT,env[key].agents)\n",
    "        env[key].add_agent(best_ddqn_agent.get_name())\n",
    "        env[key].set_decision_agent(best_ddqn_agent.get_name())\n",
    "        best_ddqn_agent.set_environment(rl_env)\n",
    "        best_ddqn_agent.train(start_idx = trn_idx[key][0],\n",
    "                    end_idx=trn_idx[key][1],\n",
    "                    training_episodes= 500,\n",
    "                    epsilon_decya_func= linear_decay,\n",
    "                    initial_epsilon= 0.9,\n",
    "                    final_epsilon= 0.1,\n",
    "                    update_q_freq= 1,\n",
    "                    update_tgt_freq= 10,\n",
    "                    save_path = export_path,\n",
    "                    val_start_idx = val_idx[key][0],\n",
    "                    val_end_idx = val_idx[key][1],\n",
    "                    metric_func= metric_function,\n",
    "                    min_training_episodes = 100, \n",
    "                    early_stop = True,\n",
    "                    stop_metric = 'val_tot_r',\n",
    "                    stop_patience = 20,\n",
    "                    stop_delta = 0.001)\n",
    "    \n",
    "        ## Export Training Session Data to CSV\n",
    "        ddqn_trn = best_ddqn_agent.get_training_episodic_data()\n",
    "        ddqn_trn.to_csv('test.csv')\n",
    "        display(ddqn_trn)\n",
    "        env[key].remove_agent(best_ddqn_agent.get_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_function(env):\n",
    "    metric = env.step_info[-1]['New Portfolio Value'] -  env.step_info[-1]['Portfolio Value']\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, rl_env in env.items():\n",
    "    \n",
    "    if key in trn_keys:\n",
    "        timesnet.upload_csv(csv_path + stock_tn[key])\n",
    "        print(env[key].agent_sequence,env[key].DECISION_AGENT,env[key].agents)\n",
    "        env[key].add_agent(best_ddqn_agent.get_name())\n",
    "        env[key].set_decision_agent(best_ddqn_agent.get_name())\n",
    "        best_ddqn_agent.set_environment(rl_env)\n",
    "        best_ddqn_agent.train(start_idx = trn_idx[key][0],\n",
    "                    end_idx=trn_idx[key][1],\n",
    "                    training_episodes= 100,\n",
    "                    epsilon_decya_func= linear_decay,\n",
    "                    initial_epsilon= 0.9,\n",
    "                    final_epsilon= 0.1,\n",
    "                    update_q_freq= 1,\n",
    "                    update_tgt_freq= 10,\n",
    "                    save_path = export_path,\n",
    "                    val_start_idx = val_idx[key][0],\n",
    "                    val_end_idx = val_idx[key][1],\n",
    "                    metric_func= metric_function,\n",
    "                    min_training_episodes = 25, \n",
    "                    early_stop = True,\n",
    "                    stop_metric = 'val_tot_r',\n",
    "                    stop_patience = 7,\n",
    "                    stop_delta = 0.001)\n",
    "    \n",
    "        ## Export Training Session Data to CSV\n",
    "        ddqn_trn = best_ddqn_agent.get_training_episodic_data()\n",
    "        ddqn_trn.to_csv('test.csv')\n",
    "        display(ddqn_trn)\n",
    "        env[key].remove_agent(best_ddqn_agent.get_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, rl_env in env.items():   \n",
    "    test_name_path = f'{export_path}/{best_ddqn_agent.get_name()}'       \n",
    "    if key in tst_keys:\n",
    "        \n",
    "        test_name = f'{best_ddqn_agent.get_name()}-TST-{key}-{tst_idx[key][0]}_{tst_idx[key][0]}'\n",
    "        timesnet.upload_csv(csv_path + stock_tn[key])\n",
    "        env[key].add_agent(best_ddqn_agent.get_name())\n",
    "        env[key].set_decision_agent(best_ddqn_agent.get_name())\n",
    "        print(f'Seq: {env[key].agent_sequence}, DA: {env[key].DECISION_AGENT}, Agents: {env[key].agents}')\n",
    "        best_ddqn_agent.set_environment(rl_env)\n",
    "        best_ddqn_agent.test(start_idx = tst_idx[key][0],\n",
    "                             end_idx = tst_idx[key][1],\n",
    "                             metric_func= metric_function,\n",
    "                             testing_episodes=1)\n",
    "    \n",
    "    \n",
    "        ## Export Test data\n",
    "        a = env[key].get_step_data()\n",
    "        b = best_ddqn_agent.get_step_data()\n",
    "        combined_df = pd.concat([a,b],axis=1)\n",
    "        tst_df_file_name  = f'{test_name}.csv'\n",
    "        trn_df_save_path = test_name_path + '/' + tst_df_file_name\n",
    "        combined_df.to_csv(trn_df_save_path)\n",
    "\n",
    "        ## Generate Trading Graphic\n",
    "        tst_graph_file_name = trn_df_save_path[:-4] + '.png'\n",
    "        agentperform.agent_stock_performance(env[key].stock_price_data[tst_idx[key][0]:tst_idx[key][1]][:,-1,0], # Selecting all batches, last price of window, closing price\n",
    "                                            combined_df['Env Action'].to_numpy(),\n",
    "                                            key,\n",
    "                                            best_ddqn_agent.get_name(),\n",
    "                                            display_graph=True,\n",
    "                                            save_graphic=True,\n",
    "                                            path_file=tst_graph_file_name)\n",
    "        env[key].remove_agent(best_ddqn_agent.get_name())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggreate Test Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ?Final Hypertuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Objective function, need to create agent name before to link agent with enviornment\n",
    "agent_name = 'REWARD_DDQN_AGENT'\n",
    "agent_path = export_path + '/' + agent_name\n",
    "metric = 'val_ror'\n",
    "\n",
    "for key, env in environments.items():\n",
    "  \n",
    "        env.add_agent(agent_name)\n",
    "        env.set_decision_agent(agent_name)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # Define the hyperparameters to search over\n",
    "    \n",
    "    ## NN hyperparameters\n",
    "    sug_hidden_layers = trial.suggest_int('hidden_layers', low=1, high=3)\n",
    "    sug_hidden_size = trial.suggest_int('hidden_size', low=256, high=1280, step=64)\n",
    "    sug_update_q_freq = trial.suggest_int('update_q_freq',low=1,high=5)\n",
    "    sug_update_tgt_freq = trial.suggest_int('update_tgt_freq',low=10,high=50,step=10)\n",
    "    \n",
    "    ## Function Passing\n",
    "    activation_functions = {\n",
    "    'RELU': nn.ReLU(),\n",
    "    'LRELU': nn.LeakyReLU(),\n",
    "    'GELU': nn.GELU(),\n",
    "    'TANH': nn.Tanh()\n",
    "    }\n",
    "    sug_activation_function_name = trial.suggest_categorical('activation_function', list(activation_functions.keys()))\n",
    "    sug_activation_function = activation_functions[sug_activation_function_name]\n",
    "    \n",
    "    ## Optimizer hyperparameters\n",
    "    sug_opt_lre = trial.suggest_float('opt_lre',0.0001,0.1,log=True)\n",
    "    ## Memory Replay hyperparameters\n",
    "    sug_buffer_size = trial.suggest_int('buffer_size',low=100,high=1500,step=100)\n",
    "    sug_batch_size = trial.suggest_int('batch_size',low=10,high=150,step=10)\n",
    "\n",
    "    # Saving Setup\n",
    "    ## Current Parameter Values:\n",
    "    cur_n_fcl = trial.params['hidden_layers']\n",
    "    cur_fcl_size = trial.params['hidden_size']\n",
    "    cur_q_freq = trial.params['update_q_freq']\n",
    "    cur_tgt_freq = trial.params['update_tgt_freq']\n",
    "    cur_act_func = trial.params['activation_function']\n",
    "    cur_lre = decimal_to_text(trial.params['opt_lre'])\n",
    "    cur_buf_size = trial.params['buffer_size']\n",
    "    cur_bat_size = trial.params['batch_size']\n",
    "    \n",
    "    ## Create Notation for Hyperparameter Setup    \n",
    "    test_name = (f'{cur_n_fcl}FC{cur_fcl_size}_{cur_act_func}_' +\n",
    "                f'BT{cur_bat_size}BF{cur_buf_size}_Q{cur_q_freq}_' +\n",
    "                f'TGT{cur_tgt_freq}_LR{cur_lre}')\n",
    "    \n",
    "    ## Create Dir to save results\n",
    "    test_name_path =  agent_path + '/' + test_name \n",
    "    if not os.path.exists(test_name_path):\n",
    "        os.makedirs(test_name_path)\n",
    "        print(f\"Directory '{test_name_path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory '{test_name_path}' already exists.\")    \n",
    "    \n",
    "    # Create Agent with hyperparameters  \n",
    "    best_ddqn_agent = DdqnAgent(name=agent_name,\n",
    "                        environment=None,\n",
    "                        reward_function = future_profit,\n",
    "                        reward_params = {'n':5},\n",
    "                        env_state_mod_func = flatten_state,     \n",
    "                        input_size= 13,\n",
    "                        hidden_size= sug_hidden_size, \n",
    "                        output_size=3, \n",
    "                        activation_function = sug_activation_function,\n",
    "                        num_hidden_layers = sug_hidden_layers,                  \n",
    "                        buffer_size= sug_buffer_size, \n",
    "                        batch_size = sug_batch_size,\n",
    "                        opt_lr= sug_opt_lre,\n",
    "                        alpha = ALPHA,\n",
    "                        gamma = GAMMA,\n",
    "                        opt_wgt_dcy = 0.0,\n",
    "                        dropout_rate = 0.25,                \n",
    "                        device = device)\n",
    "\n",
    "    # Training Model\n",
    "    for key, env in environments.items():\n",
    "        \n",
    "        if key in trn_keys:\n",
    "            \n",
    "            best_ddqn_agent.set_environment(env)\n",
    "            best_ddqn_agent.train(start_idx=training_range[0],\n",
    "                        end_idx=training_range[1],\n",
    "                        training_episodes= 100,\n",
    "                        epsilon_decya_func= linear_decay,\n",
    "                        initial_epsilon= 0.9,\n",
    "                        final_epsilon= 0.1,\n",
    "                        update_q_freq= sug_update_q_freq,\n",
    "                        update_tgt_freq= sug_update_tgt_freq,\n",
    "                        save_path = export_path,\n",
    "                        val_start_idx = validation_range[0],\n",
    "                        val_end_idx = validation_range[1],\n",
    "                        early_stop = True,\n",
    "                        stop_metric = metric,\n",
    "                        stop_patience = 20,\n",
    "                        stop_delta = 0.001)\n",
    "        \n",
    "            ## Export Training Session Data to CSV\n",
    "            ddqn_trn = best_ddqn_agent.get_training_episodic_data()\n",
    "            trn_df_file_name  = f'TRN-{key}' + test_name + '.csv'\n",
    "            trn_df_save_path = test_name_path + '/' + trn_df_file_name\n",
    "            ddqn_trn.to_csv(trn_df_save_path)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Test Model\n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "    for key, env in environments.items():\n",
    "    \n",
    "        if key in tst_keys:\n",
    "            \n",
    "            best_ddqn_agent.set_environment(env)              \n",
    "            best_ddqn_agent.test(start_idx = testing_range[0],\n",
    "                        end_idx = testing_range[1], \n",
    "                        testing_episodes=1)\n",
    "\n",
    "            ## Save Test Metric Result(s) into \n",
    "            ddqn_tst = best_ddqn_agent.get_testing_episodic_data()\n",
    "            score = ddqn_tst['Total Reward'].mean()\n",
    "            scores.append(score)\n",
    "    \n",
    "            ## Export Test data\n",
    "            a = env.get_step_data()\n",
    "            b = best_ddqn_agent.get_step_data()\n",
    "            combined_df = pd.concat([a,b],axis=1)\n",
    "            tst_df_file_name  = f'TST-{key}' + test_name + '.csv'\n",
    "            trn_df_save_path = test_name_path + '/' + tst_df_file_name\n",
    "            combined_df.to_csv(trn_df_save_path)\n",
    "\n",
    "            ## Generate Trading Graphic\n",
    "            tst_graph_file_name = trn_df_save_path[:-4] + '.png'\n",
    "            agentperform.agent_stock_performance(env.stock_price_data[testing_range[0]:testing_range[1]],\n",
    "                                                combined_df['Env Action'].to_numpy(),\n",
    "                                                key,\n",
    "                                                best_ddqn_agent.get_name(),\n",
    "                                                display_graph=True,\n",
    "                                                save_graphic=True,\n",
    "                                                path_file=tst_graph_file_name)\n",
    "\n",
    "    mean = np.mean(scores)\n",
    "    return mean\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best value: \", study.best_value)\n",
    "print(\"Best params: \", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MADDQN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
